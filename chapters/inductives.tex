\setchapterimage[6cm]{seaside}
\setchapterpreamble[u]{\margintoc}
\chapter{Optional Features of \SetoidCC}
\labch{extensions}

In this chapter, we investigate several variants and extensions of \SetoidCC.

In \cref{sec:cast-refl}, we add a computation rule to the \( \castName \)
operator so that it reduces when applied to a reflexive identity proof. 
We propose a normalization algorithm for the resulting system, and we 
conjecture that it always terminates on well-typed terms.

Then in \cref{sec:heterogeneous}, we go over two alternative presentations of the 
observational equality: a heterogeneous equality and an equality with no
reduction rules.

In \cref{sec:general-inductives}, we explain how to add general indexed 
inductive types in \SetoidCC, and we discuss some difficulties with parameters.

Finally, in \cref{sec:proof-rel-impred} we add a universe \( \Prop \) of 
proof-\emph{relevant} impredicative types to \SetoidCC to increase
its computational power.
% 
We show that if we extend the observational equality and the \( \castName \)
operator to this universe, type-checking becomes undecidable.

\section{Cast and Reflexivity}
\label{sec:cast-refl}

\subsection{Observational versus Inductive}

In \SetoidCC, we have a bit of an awkward trade-off between two different 
notions of propositional equality.

On the one hand, we have the observational equality \( \Obseq[A]{t}{u} \),
a proof-irrelevant proposition.
% 
The observational equality has strong selling points: not only does it validate 
the principle of Uniqueness of Identity Proofs by definition thanks 
computational irrelevance, but it is also equipped with reduction rules that 
bake in useful reasoning principles such as function extensionality and proposition 
extensionality.

But being computationally irrelevant also has its drawbacks. Because the equality
proofs carry no information, the eliminator for the observational equality 
computes on types constructors instead. This means that \( \castName \) will 
only reduce fully when it is applied to closed types.
% 
To make up for this weakness, \SetoidCC provides the axiom \( \castreflName \) 
which is a proof of observational equality between a reflexive cast and the
identity.
% 
\sideremark{Note that the rule for \( \castreflName \) is phrased with an 
  arbitrary proof of equality between \( A \) and itself. This is equivalent
  to the statement using \( \metaop{refl} \) because of proof irrelevance.}
% 
\begin{mathpar}
	\inferrule[Cast-Refl]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{A}}{\sProp}}
		{\tytmannotated{\Gamma}{\castrefl{A}{t}}{\Obseq[A]{t}{\cast{A}{A}{e}{t}}}{\sProp}}
\end{mathpar}

On the other hand, we have the inductive equality \( \Id{A}{t}{u} \), which is
a proof-relevant analogue of the Martin-Löf identity type --
% 
and as such, it comes equipped with the usual \( \metaop{J} \) eliminator and its
computation rule. 
% 
\begin{mathpar}
  \inferrule{\tytm{\Gamma}{A}{\Type_i}
			\\ \tytm{\Gamma}{t}{A}
			\\ \tytm{\Gamma}{B}{\Depfun{A}{\Fun{\Id{A}{t}{x}}{\Type_j}}}
			\\ \tytm{\Gamma}{u}{B\ t\ \id{t}}}
			{\red{\Gamma}{\J{A}{t}{B}{u}{t}{\id{t}}}{u}{B\ t\ \id{t}}}
\end{mathpar}

But it is also better than the Martin-Löf identity type, as it validates the 
signature reasoning principles of \SetoidCC that are UIP, function 
extensionality and proposition extensionality.
% 
However, computing on reflexivity comes at a price: for the inductive equality,
the principle of UIP only holds up to a propositional equality, and the same 
goes for function extensionality and proposition extensionality.

All in all, the difference is rather subtle. The two relations are logically 
equivalent, but they do not satisfy the exact same computation rules.
% 
As if the distinction between propositional and definitional equality 
wasn't a sufficient source of confusion for newcomers to type theory!

\subsection{The System \SetoidCCplus}

In hope to reconciliate these two propositional equalities, we introduce a 
variation on the observational calculus of constructions, that we call 
\SetoidCCplus. 
% 
This new system supports all the rules of \cref{ch:observational}, plus the rule 
\nameref{inferrule:cast-refl-plus} which promotes the \( \castreflName \) axiom
to a definitional equality.
% 
\begin{mathpar}
	\inferrule[Cast-Refl+]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{A}}{\sProp}}
		{\eqtmannotated{\Gamma}{\cast{A}{A}{e}{t}}{t}{A}{\Type_i}}
  \ilabel{inferrule:cast-refl-plus}
\end{mathpar}

In \SetoidCCplus, we can define a \( J \) eliminator for the observational
equality that computes on reflexivity.
% 
\sideremark{ Recall that we defined \\ \( J(A, t, B, u, t', e) := \) \\
  \( \cast{B\ t\ \metaop{refl}}{B\ t' e}{\eqJ{A}{t}{B}{t'\!}{e}}{u} \)}
% 
To do so, we simply replicate the definition described in 
\cref{sec:inductive-equality}, and in presence of 
rule~\nameref{inferrule:cast-refl-plus} it will actually satisfy the 
computation rule on reflexivity as a definitional equality.
% 
Therefore, the observational equality of \SetoidCCplus really has the best
of both worlds: not only is it definitionally proof irrelevant, but its eliminator 
also supports the computation rule on reflexivity by definition.

Naturally, extending the convertibility relation of \SetoidCC means that we
need a new algorithm to decide conversion for \SetoidCCplus terms.
% 
We will describe two such algorithms below, but unfortunately we are not
able to supplement either with a proof of correction. 
We leave these as conjectures for further work.

\subsection{Implementing \nameref{inferrule:cast-refl-plus} with reduction rules}

The first idea that comes to mind is to add a new reduction rule for 
\( \cast{A}{B}{e}{t} \) so that it will actually reduce to \( t \) in case the
types \( A \) and \( B \) are convertible.
% 
\begin{mathpar}
	\inferrule[Cast-Refl-Red]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
      \tytm{\Gamma}{B}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{B}}{\sProp} \\
      \eqtm{\Gamma}{A}{B}{\Type_i}}
		{\red{\Gamma}{\cast{A}{B}{e}{t}}{t}{A}}
  \ilabel{inferrule:cast-refl-red}
\end{mathpar}

This is a bit too enthusiastic however, as adding such a general rule to the 
system conflicts with the reduction rules for \( \castName \) on type 
constructors.
% 
For instance, in the case of a \( \castName \) between two
convertible \( \Pi \)-types, both rule \nameref{inferrule:cast-refl-red} and 
\nameref{inferrule:cast-pi} apply.

\sideremark{Remark that this critical pair can be unified, but only by 
reducing under a binder, which is not allowed in the weak-head reduction 
strategy. In fact this is the case for all critical pairs introduced by
rule \nameref{inferrule:cast-refl-red}, meaning that if we could devise a
deep reduction strategy for \SetoidCCplus that is non-deterministic but confluent.}
\begin{center}
\begin{tikzpicture}
  \node[label=above:{\( \cast{\Depfun{A}{B}}{\Depfun{A}{B}}{\metaop{refl}}{f} \)}] (A) at (0, 0) {} ;
  \node[label=south west:{\( f \)}] (B) at (-2, -0.5) {} ;
  \node[label=below:{\( \begin{array}{l} \lambda\ x\ .\ \metaop{cast}({\subst{B}{a}}, {\subst{B}{a}},
    \\ \phantom{\lambda\ x\ .\ \metaop{cast}}{\metaop{refl}}, {f\ \cast{A}{A}{\metaop{refl}}{x}}) \end{array} \)}]
    (C) at (2, -0.5) {} ;
  \draw [double distance=1.5pt, -{Implies}] (1,0) -- (2,-0.5) ;
  \draw [double distance=1.5pt, -{Implies}] (-1,0) -- (-2,-0.5) ;
\end{tikzpicture}
\end{center}

Since rule~\nameref{inferrule:cast-refl-red} is admissible for closed terms
and interferes with the reduction rules on constructors, we can restrict
its application to situations where \( A \) and \( B \) are neutral types.
% 
This way, we recover a deterministic reduction strategy, although we lose 
stability of weak-head reduction under substitution.
% 
\begin{mathpar}
	\inferrule[Cast-Refl-Neutral]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
      \tytm{\Gamma}{B}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{B}}{\sProp} \\
      \eqtm{\Gamma}{A}{B}{\Type_i} \\
      \text{\( A \) and \( B \) neutral}}
		{\red{\Gamma}{\cast{A}{B}{e}{t}}{t}{A}}
  \ilabel{inferrule:cast-refl-neutral}
\end{mathpar}

Still, even the determinized rule~\nameref{inferrule:cast-refl-neutral} is by no 
means innocuous, as it dramatically changes the behavior of the system: 
% 
unlike all the other reduction rules of \SetoidCCplus, it is not triggered by 
a redex consisting of a destructor applied to a constructor, but rather by a 
convertibility premise.
% 
As a consequence, weak-head reduction is now deeply intertwined with conversion
checking. When computing the head normal form of a term, it might be necessary
to check convertibility of two terms, which requires putting them in weak head 
normal form, etc.

\paragraph*{Normalization}

Type casting operators that reduce on identical types have a long history in
type theory. 
% 
In the context of impredicative systems, they were notably studied by Girard 
for System F \sidecite{girard72}, and by Werner for \CIC \sidecite{Werner2008}.
% 
In both cases, it unfortunately turns out that the resulting system loses 
normalization for open terms \sidecite{lmcs:6606}.

This incompatibility between impredicativity and type casting can be 
interpreted as a consequence of the breakage of parametricity.
%
Indeed, the operator \( {\castName : (A\ B\ : \Type_i) \to A \to B} \) 
is fundamentally anti-parametric, because in case \( A \equiv B \) it
reduces to \( t \) which satisfies the parametricity predicate associated 
to \( A \) but not necessarily the predicate associated to \( B \).
% 
But normalization models for impredicative type theories are usually built
using reducibility candidates, which rely crucially on parametricity to 
work.

Fortunately, we expect that our system \SetoidCCplus will avoid these issues, 
because \( \castName \) only computes in the predicative fragment and 
proof-irrelevance once again comes to our rescue in the impredicative 
layer.
% 
Therefore, we feel justified in conjecturing that this system is in fact 
normalizing.
% 
\begin{conjecture}
  \SetoidCCplus with computation rules for \( \castName \) on neutral types is 
  normalizing.
\end{conjecture}

As further evidence for this conjecture, we remark that the situation is very 
similar to reduction rules for \( \castName \) on type constructors, 
which are certainly not parametric either.
% 
In fact, in \cref{sec:proof-rel-impred} we will show that extending these rules 
to a proof-relevant impredicative universe breaks normalization for the exact 
same reason.
% 
Nonetheless, by restricting computation to the predicative layer, we were able 
to prove normalization for \SetoidCC in \cref{ch:metatheory}. 

\subsection{Implementing \nameref{inferrule:cast-refl-plus} in conversion checking}

In conclusion to the previous section, our attempt at implementing 
rule~\nameref{inferrule:cast-refl-plus} with reduction is not a resounding 
success.
% 
The resulting reduction strategy cannot be defined independently from 
conversion checking, and it is not even stable under substitution.
% 
At that point, it is not clear that we gain much by performing said reductions.

We are tempted to draw a parallel with \( \eta \)-equality of functions. 
% 
Just like rule~\nameref{inferrule:cast-refl-plus}, the \( \eta \)-equality rule 
does not fit the general pattern of reducing a redex, and it does not lend 
itself too well to implementation \textit{via} reduction rules either -- both the
\( \eta \)-expansion and the \( \eta \)-reduction ideas are unsatisfying
\sidecite{meven_eta}.
% 
\sideremark{This is also how our conversion checking algorithm from 
\cref{ch:metatheory} proceeds.}
In practice, all major proof assistants chose to implement \( \eta \) during 
the conversion checking phase: when the algorithm needs to decide 
convertibility between a function and a neutral term, it performs an \( \eta \)
expansion of the neutral term and calls itself recursively.

Therefore, we suggest to keep the well-behaved reduction rules of \SetoidCC,
and to handle rule~\nameref{inferrule:cast-refl-plus} during conversion checking.
% 
When the algorithm needs to decide convertibility between a term of the form 
\( \cast{A}{B}{e}{t} \) where \( A \) and \( B \) are two neutral terms, and a
term \( u \) in weak-head normal form that is not a \( \castName \), the 
algorithm starts by deciding convertibility of \( A \) and \( B \). 
% 
In case \( A \) and \( B \) are indeed convertible, then the algorithm can 
remove the \( \castName \) and simply call itself recursively \( t \) and 
\( u \).
% 
If \( A \) and \( B \) are not convertible, then the algorithm gives a negative 
answer.

\begin{conjecture}
  The algorithm sketched above decides conversion for \SetoidCCplus.
\end{conjecture}

We leave the investigation of this question for future works.

\section{Variations on the Observational Equality}
\label{sec:heterogeneous}

\subsection{A Proof-Irrelevant Heterogeneous Equality}

In \SetoidCC, we can form the observational equality of two terms only when 
they have the same type. 
% 
In other words the observational equality is homogeneous -- just like the 
definitional equality, the inductive equality, or most equalities routinely 
considered by type theorists.
% 
But when they introduced Observational Type Theory, Altenkirch, McBride and 
Swierstra decided to use a heterogenous 
equality~\sidecite{altenkirchAl:plpv2007}, arguing that it results in simpler
rules.

It is not too difficult to modify the presentation of \SetoidCC so that it uses a
proof-irrelevant heterogeneous equality \textit{à la} Altenkirch \etal instead 
of the homogeneous observational equality.
% 
First, we change the formation rule of the equality so that it applies between
two terms with different types:
% 
\begin{mathpar}
  \inferrule[Heterogenous-Eq-Form]
    {\tytmannotated{\Gamma}{t}{A}{\Type_i}
    \\ \tytmannotated{\Gamma}{u}{B}{\Type_i}}
    {\tytm{\Gamma}{\Heteq{t}{A}{u}{B}}{\sProp}}
  \and
\end{mathpar}

Note that unlike the ``path-over'' equality of cubical type theory, we do not 
ask for a proof of equality between \( A \) and \( B \) to form the type
\( \Heteq{t}{A}{u}{B} \).
% 
We also replace rules \nameref{inferrule:refl},
\nameref{inferrule:transport-prop} and \nameref{inferrule:cast} with the 
following:
\begin{mathpar}
  \inferrule[Heterogeneous-Transport-\( \sProp \)]
		{%\tytm{\Gamma}{A}{\Type_i}
    \tytmannotated{\Gamma}{t}{A}{\Type_i}
    \\ \tytm{\extctx[x]{\extctx[X]{\Gamma}{\Type_i}}{X}}{B}{\sProp} 
    \\ \tytmannotated{\Gamma}{u}{\subst[x]{\subst[X]{B}{A}}{t}}{\sProp}
    \\ \tytmannotated{\Gamma}{t'}{A'}{\Type_i}
    \\ \tytmannotated{\Gamma}{e}{\Heteq{t}{A}{t'}{A'}}{\sProp}}
  {\tytmannotated{\Gamma}{\hettransport{A}{t}{B}{u}{A'}{t'}{e}}{\subst[x]{\subst[X]{B}{A'}}{t'}}{\sProp}}
\end{mathpar}
\begin{mathpar}
	\inferrule[Heterogeneous-Cast]
		{% \tytm{\Gamma}{A}{\Univ_i}   \quad \tytm{\Gamma}{B}{\Univ_i}
			\tytmannotated{\Gamma}{e}{\Heteq{A}{\Univ}{B}{\Univ}}{\sProp}
			\quad \tytmannotated{\Gamma}{t}{A}{\Univ}}
		{\tytmannotated{\Gamma}{\cast{A}{B}{e}{t}}{B}{\Univ}}
  \and
  \inferrule[Heterogeneous-Refl]
    {\tytmannotated{\Gamma}{t}{A}{\Type_i}}
    {\tytmannotated{\Gamma}{\refl{t}{t}}{\Heteq{t}{A}{t}{A}}{\sProp}}
\end{mathpar}

From a computational perspective, the heterogeneous equality is similar
to the homogeneous equality.
% 
The type \( \Heteq{t}{A}{u}{B} \) is an eliminator that computes by comparing 
the head constructors of the types \( A \) and \( B \). 
% 
If the constructors match, the heterogeneous equality follows the computation rule that
describes equality for the corresponding type former, and if they do not match 
the proposition reduces to \( \Empty \)
\sideremark{Reducing to \( \Empty \) on non-matching types is optional. It is 
  a reasonable axiom, but it also adds an unnecessary constraint on the models 
  of the theory.}.
 
Naturally, we must also adjust the reduction rules that describe equality on the 
type formers. For instance, the heterogeneous equality between two dependent pairs 
now reads as follows:
% 
\begin{mathpar}
  \inferrule[Heterogeneous-Eq-Pair]{\tytmannotated{\Gamma}{t,u}{\Depsumannotated{A}{B}{}{\Type_i}{\Type_j}}{\Type_{\mathrm{max}(i,j)}}} 
			{\redmultiline{\Gamma}{\Heteq{t}{\Sigma A B}{u}{\Sigma A B}}{(\Heteq{\relfst{t}}{A}{\relfst{u}}{A})
				\land {(\Heteq{\relsnd{t}}{B[\relfst{t}]}{\relsnd{u}}{B[\relfst{u}]})}}{\sProp}}
\end{mathpar}

Remark that contrary to rule \nameref{inferrule:eq-pair}, we do not need to use
type casting to state equality between the second projections. 
% 
This is exactly what makes the heterogeneous equality nicer than the homogeneous 
equality: it can be defined independently from \( \castName \).

Other than making the system more modular, replacing the observational equality with the 
heterogeneous variant does not change the behavior of \SetoidCC too much. 
% 
It seems safe to assume that all the meta-theory that we developed in
\cref{ch:metatheory} would work just as well in this context.
Thus the choice between homogeneous and heterogeneous is a matter of taste
more than anything else.

\subsection{Equality Without Computation}

The exact nature of the observational equality does not matter
much.
% 
\sideremark{Two types are incompatible if their head constructors are 
  different. Since \( \castName \) between incompatible types are stuck terms, we want
  to make sure they can't be formed in the empty context, lest we get 
  non-canonical integers.}
% 
As a computationally irrelevant property, its sole purpose is to add a 
logical constraint to \( \castName \) so that it becomes impossible to cast 
between two incompatible types in an empty context.

Therefore, any alternative definition fine as long as it does not add 
equalities between incompatible types in an empty context, and as long as it allows us to
give a type to all the reduction rules for \( \castName \). 
% 
For instance, a \( \castName \) between two dependent sums reduces to a pair of 
\( \castName \) (rule~\nameref{inferrule:cast-sigma}), which means that in 
particular, we need to derive a proof of \( \Obseq[\Type]{A}{A'} \) from a proof of 
\( \Obseq[\Type]{\Depsum{A}{B}}{\Depsum{A'}{B'}} \).

But there is really nothing that requires these derivations to come from 
reduction rules. In fact, we can completely do away with the computation rules
for the observational equality, and replace them with proof-irrelevant axioms
-- for instance the following two axioms for equality on dependent sums:
\[
\begin{array}{lcl}
\metaop{eq{-}fst} & : & \Obseq[\Type]{\Depsum{A}{B}}{\Depsum{A'}{B'}} \enskip \to \enskip \Obseq[\Type]{A}{A'} \\
\metaop{eq{-}snd} & : & \Pi (e : \Obseq[\Type]{\Depsum{A}{B}}{\Depsum{A'}{B'}}) \enskip . \\
& & \enskip \Pi (a : A)\ .\ \Obseq[\Type]{\subst{B}{a}}{\subst{B'}{\cast{A}{A'}{\metaop{eq{-}fst}\ e}{a}}}.
\end{array}
\]

And the resulting system still enjoys pretty much all the properties of 
\SetoidCC, except that the proof of function extensionality is not given by 
reflexivity anymore.
% 
Avoiding computation rules for equality may be a good idea for implementation 
in a proof assistant, as they tend to make terms grow in size unnecessarily.

\section{General Inductive Types}
\label{sec:general-inductives}

\subsection{Parameters and Universes}

\section{Proof-Relevant Impredicativity}
\label{sec:proof-rel-impred}

In dependent type theory, a sort \( \sProp \) is said to be
\emph{impredicative} if it is closed under dependent products over any index
type: for all types \( A \) and functions \( \tm{B}{A \to \sProp} \),
the dependent product \( \Depfun{A}{B\ x} \) is in \( \sProp \).
%
In particular, impredicativity allows the definition of self-referential propositions,
which quantify over the type \( \sProp \) of all propositions and may thus
be applied to themselves.
%
In addition to providing a tremendous amount of logical power, impredicativity is
a crucial ingredient in common mathematical constructions, such as
Tarski's fixed point theorem or lattice theory~\sidecite{paco}.
%
On the other side of the coin, a predicative hierarchy $(\varType_i)_{i \in \Nat}$
requires dependent products to inhabit a higher universe level
than their domain and codomain: for all types \( A : \varType_i \) and functions
\( \tm{B}{A \to \varType_j} \), the dependent product
\( \Depfun{A}{B\ x} \) is in \( \varType_{max(i,j)} \).
%
While the latter is easier to model, as the universes $\varType_i$ can be
constructed incrementally by induction on the level $i$, it results
in a theory that is less flexible in practice, whether the mention of
levels is explicit (as in \Agda) or implicit (as in \Coq).
%
Impredicative propositions make for an altogether
more comfortable framework, in which less universe levels have to
be dealt with.

While impredicativity is a prominent feature of the Calculus of
Constructions~\sidecite{Coquand-CC}, and by extension of \Coq, it is
absent from the standard presentation of Martin-Löf type theory
(\MLTT~\sidecite{MARTINLOF197573}), and not available in \Agda.
%
This reluctance may be explained by the sheer difficulty of designing models
to reason about impredicative theories, and by the numerous incompatibility
results, from \sidecitet{hurkens95} paradox to the more recent proof by
\sidecitet{lmcs:6606} that a naive implementation of uniqueness of identity
proofs (UIP) via definitionally proof-irrelevant equalities breaks \Coq's
normalization algorithm on impredicative propositions.

However, as noted by Abel and Coquand, it is not clear whether that last
result stems from a deep incompatibility between UIP and impredicativity,
or if it is an artifact of an inadequate type-checking algorithm.
%
Clarifying this issue seems especially important given the renewed
interest in definitional proof-irrelevance and UIP shown by the community
in recent years~\sidecite{sterling_et_al:LIPIcs:2019:10538,pujet:hal-03367052,Altenkirch2019}.
%
For instance, the recent presentation \SetoidTT of observational
equality of \sidecitet{pujet:hal-03367052} has an equality which can be
eliminated using a cast operator that computes differently from the
usual eliminator of Martin-Löf identity type, as used by \sidecitet{lmcs:6606}.


In this paper, we show that impredicativity is harmless when confined
to a sort of definitionally proof-irrelevant propositions, as it is never
necessary to compute with irrelevant proof terms.
%
As a result, we are able to extend \SetoidTT with impredicativity while
preserving definitional UIP and all extensionality principles that come
with an observational type theory (such as propositional or function
extensionality).
%
The resulting system, dubbed \SetoidCC, still enjoys
normalization via inductive-recursive logical relations---no
realizability model is needed---and decidability of type-checking.
%
This result has been formalized in \Agda using the framework of
\sidecitet{Abel:POPL2018}.\footnote{see anonymous supplementary
  materials.}
%
The fact that a proof scheme that has been designed with predicative
theories in mind carries through in an impredicative setting
may come as a surprise.
%
The crux of the proof is to realize that we can get away with
remarkably little structure in the interpretation of the impredicative
dependent products and sums, to the extent that we do not even mention the
interpretation of the domain and codomain.
%
We manage to prove the soundness of our normalization model
despite this weaker induction hypothesis by using a \emph{paranoid}
version of the type system, which can then be proven equivalent to the
\emph{economic} version (a terminology introduced by
\sidecitet{andrej17:paranoid}).

%
Furthermore, we prove that normalization for \SetoidCC can be
carried in plain Martin-Löf type theory with indexed inductive types,
thereby showing that this impredicative universe does not contribute
to the computational power of the proof-relevant fragment at all,
despite the increase in logical power.
%
We also investigate the possibility of recovering the lost computational power
from a proof-relevant impredicative universe, hoping that the different
computational behaviour of \SetoidCC would allow us to circumvent Abel and
Coquand's argument, but alas, it does not. In fact, a slightly modified
argument shows undecidability of type-checking in presence of a
proof-relevant impredicative universe.
%
Therefore, we exhibit a tradeoff between the additional comfort of
UIP and extensionality principles, or the possibility to compute with
impredicative functions.

Finally, because our normalization proof does not imply logical
consistency, we define a model of \SetoidCC in ZF Set
theory.
%
This shows that the computational content of \SetoidCC can be studied
without impredicativity in the metatheory, while the study of the
logical content of \SetoidCC fundamentally requires impredicativity in
the metatheory.

\paragraph{Plan of the paper}

In \cref{sec:an-impr-type}, we present the syntax and typing rules of
\SetoidCC, focusing on the differences with respect to \SetoidTT.
%
Then, we develop the logical relation
framework that shows normalization and decidability of conversion for
\SetoidCC (\cref{sec:logic-relat-with}).
%
In \cref{sec:analys-norm-proof}, we show that \SetoidCC and \MLTT with
indexed inductive types can
actually express the exact same integer functions
  as closed terms of type \( \Fun{\Nat}{\Nat} \) by simplifying the
  logical relation framework to avoid the use of induction-recursion.
  %
  Finally, we show logical consistency of \SetoidCC in
  \cref{sec:cons-seto-model} and adapt the undecidability argument in
  presence of a proof-relevant impredicative universe of
  \sidecitet{lmcs:6606} to our setting in \cref{sec:setoidcc-with-an}.

  \section{\SetoidCC with an imprecative universe has undecidable type-checking}
  \label{sec:setoidcc-with-an}
  
  The system we have presented so far has an impredicative universe
  \( \sProp \) of \emph{strict} propositions, which contains the observational
  equality and the false proposition with the corresponding large
  elimination principles.
  %
  In other words, \( \sProp \) provides the user with a complete Heyting
  algebra of truth values, as is frequently assumed even in constructive
  mathematics.
  % TO BE DONE \sidecite{TODO}.
  
  However, \( \sProp \) is still a bit weaker than the impredicative
  universe \( \Prop \) of the Calculus of Inductive Constructions:
  \( \Prop \) allows large elimination for all the \emph{singleton}
  inductive types, that is the inductive types with only one constructor
  whose arguments all have sort \( \Prop \).
  %
  This includes equality, but also the accessibility predicate from which
  we can obtain some constructive choice principles (as discussed in \sidecite{forster:LIPIcs:2021:13455} that may
  come in handy for mathematics.
  
  Thus one may wonder if we can add \( \Prop \) to \SetoidCC alongside the
  universe of strict propositions, using the same computation rules as for the
  proof-relevant fragment of \SetoidCC so that it can provide us with
  more logical power while still enjoying extensionality principles.
  %
  We need to be careful though, as past experiments of adding a
  proof-irrelevant equality to Coq that have been mentioned
  in~\sidecite{gilbert:hal-01859964} have resulted in breakage of
  normalization of open terms~\sidecite{lmcs:6606}.
  %
  
  However the argument of Abel and Coquand relies on the fact that
  equality behaves as the usual Martin-Löf Identity Type, which is an
  inductive type that computes via the \( J \)-eliminator.
  %
  In \SetoidCC, elimination of equality is done using the cast operator
  which is quite different, as it computes by comparing the normal forms
  of the source and target types, which intertwines normalization and
  conversion checking in sophisticated ways.
  %
  Unfortunately, it turns out that \SetoidCC is not compatible with \( \Prop \)
  either.
  %
  In fact, we show in this section that it is possible to use a modified
  version of Abel and Coquand's argument to show undecidability of
  type-checking for open terms in presence of \( \Prop \), both for
  \SetoidCC and the theory of \sidecitet{lmcs:6606}.
  
  \subsection{Outline}
  
  The idea of the proof is rather simple:
  %
  on the one hand, impredicativity makes it possible to give a type to the term
  \[
  \Delta_f := \lambda x .\ f\ (x\ x)
  \]
  for any well-typed function \( f \) of type \( \Fun{(\Fun{\Nat}{\Nat})}{(\Fun{\Nat}{\Nat})} \), which is one half of the fixed point
  combinator \( Y_f := \Delta_f\ \Delta_f \).
  %
  On the other hand, \( \castName \) can be used to convert between any two types
  in an inconsistent context, in a way that does not block reduction.
  This allows us to apply \( \Delta_f \) to itself (up to a cast) and obtain
  an approximation of \( Y_f \).
  %
  Then, using these pseudo-fixed points, we can build a term that loops through the
  iterates of any integer function:
  %
  \begin{theorem}
    Let \( g \) be any closed term of type \( \Fun{\Nat}{\Nat} \), where \( \Nat \)
    denotes the inductive natural numbers in \( \Prop \).
    %
    In an inconsistent context, one can define a term \( \mathrm{dec}_g \)
    of type \( \Nat \) which is convertible to 0 if and only if there is a positive integer
    \( n \) such that \( g^n\ 1 = 0 \), and diverges otherwise.
  \end{theorem}
  %
  Of course, the existence of such an integer is not decidable in general,
  thus conversion and typing are undecidable for \SetoidCC + \( \Prop \).
  %
  As an aside, note that this construction can also be carried out in
  \( \sProp \) if we add proof-irrelevant natural numbers. But it does
  not cause any issue, as we have a simple way to check for convertibility of
  proof-irrelevant terms: having the same type is sufficient! For this reason,
  we never perform reduction in the proof-irrelevant layers, and need not
  to worry about these potentially divergent terms.
  
  \subsection{Definition of an Approximate Fixed Point Combinator}
  
  Let \( f \) be a closed function of type \( \Fun{(\Fun{\Nat}{\Nat})}{(\Fun{\Nat}{\Nat})} \).
  %
  In an inconsistent context, we can derive a term
  \[
     e : \Depfun[X\ Y]{\Prop}{\Obseq{X}{Y}}
  \]
  that allows us to freely cast between any two types.
  %
  We can use it to derive our approximate fixpoint combinator by defining
  \begin{align*}
    \bot &:= \Depfun[X]{\Prop}{X}\\
    \Delta_f &:= \lam{\bot}{f\ (x\ (\Fun{\bot}{\Fun{\Nat}{\Nat}})\ x)}\\
    \Delta_f' &:= \lam[X]{\Prop}{\cast{\Fun{\bot}{\Fun{\Nat}{\Nat}}}{X}{e}{\Delta_f}}\\
    Y_f &:= \Delta_f\ \Delta_f'
  \end{align*}
  Technically, it turns out that \( Y_f \) does not exactly behave like
  a fixpoint combinator because of additional casts appearing during the
  reduction.
  %
  A tedious but straightforward computation in \SetoidCC shows that
  \begin{align*}
    Y_f &= \Delta_f\ \Delta_f'\\
        &= f\ (\alpha\ (\Delta_f\ (\beta\ \Delta_f')))\\
        &= f\ (\alpha\ (f\ (\alpha^2\ (\Delta_f\ (\beta^3\ \Delta_f')))))\\
        &= f\ (\alpha\ (f\ (\alpha^2\ (f\ (\alpha^4\ (\Delta_f\ (\beta^7\ \Delta_f')))))))\\
        &= ...
  \end{align*}
  where \( \alpha := \lambda x.\ {\cast{\Fun{\Nat}{\Nat}}{\Fun{\Nat}{\Nat}}{e}{x}} \)
  and \( \beta := \lambda x.\ {\cast{\bot}{\bot}{e}{x}} \).
  
  However, this behavior is sufficient for our purposes, since all the \( \alpha \) disappear
  when the function is applied to an actual integer.
  %
  Therefore, given a closed function \( g \) of type \( \Fun{\Nat}{\Nat} \) that
  terminates on all integer inputs, we instantiate \( f \) with
  \[
    \lam[p]{\Fun{\Nat}{\Nat}}{\lam[n]{\Nat}{{\natrec{\Nat}{0}{p\ (g\ n)}{n}}}}
  \]
  and we obtain that \( Y_f\ 1 \) reduces to 0 if there exists a \( n \) such that
  \( g^n\ 1 = 0 \), and diverges (for any reduction strategy) otherwise.
  
  
  By defining $\castName$ using the elimination principle of the
  inductive equality, the same result applies to the
  theory of \sidecitet{lmcs:6606}, even if the reduction of $Y_f$ is
  slightly different.
  
  \subsection{Conversion is Undecidable}
  
  \SetoidCC is expressive enough to encode Turing machines so that
  knowing whether there exists a \( n \) such that \( g^n\ 1 = 0 \) is
  undecidable in general.
  %
  To complete the proof of undecidability of conversion, we still need
  to show that a term that diverges for any reduction strategy is not
  convertible to $0$. In other words, we need to show that conversion is
  not degenerate.
  %
  To establish this, we consider a variation on the standard results of untyped
  lambda-calculus (\sidecite{Barendregt}) showing that diverging terms
  cannot be equal to a normal form, by using confluence of the system
  and the fact that equality of two terms $t$ and $u$ corresponds to the
  existence of a zig-zag of reduction $t \Rightarrow^* t_1 \Leftarrow^*
  t_2\Rightarrow^*\dots \Leftarrow^* u$.
  
  In \CIC, confluence of reduction has been formally proven by
  \sidecitet{coqcoqcorrect}, by using the standard Tait-Martin Löf
  method~\sidecite{takahashi1989parallel} making use of parallel
  reduction.
  %
  Extending the proof of confluence for \SetoidCC is straightforward for
  the reduction involving the cast operator and the equality type, as
  they do not add any critical pair.
  
  The only difficulty is to additionally account for $\eta$-conversion
  and proof-irrelevance in the conversion to get a similar notion of
  zig-zag.
  %
  Actually, $\eta$-conversion can be turned into an
  $\eta$-reduction rule and for proof-irrelevance, we can use the fact
  that because no reduction depends on the shape of a proof-irrelevant
  terms, proof-irrelevance commutes with reduction.
  So in \SetoidCC + $\Prop$, any proof of conversion between $t$ and $u$
  can be seen as a zig-zag up to proof-irrelevance
  %
  $t \Rightarrow^* t_1 \equiv_{pi} t_1' \Leftarrow^*
  t_2\Rightarrow ^* t_2' \equiv_{pi} \dots \Leftarrow^* u$ where $\equiv_{pi}$ is syntactic
  equality up-to applications of proof-irrelevance.
  %
  From this, using confluence, we conclude to any term that it convertible to $0$ can be
  reduced to $0$, so a term that diverges for any reduction strategy
  cannot be convertible to $0$.
  
  Again, a similar proof can be done in the theory of \sidecitet{lmcs:6606}.
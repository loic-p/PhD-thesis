% \setchapterimage[6cm]{seaside}
\setchapterpreamble[u]{\margintoc}
\chapter{Optional Features and Extensions of \SetoidCC}
\labch{extensions}

In this chapter, we investigate several variants and extensions of the
observational calculus of constructions.

In \cref{sec:cast-refl}, we add a computation rule to the \( \castName \)
operator so that it reduces when applied to a reflexive identity proof. 
We propose a normalization algorithm for the resulting system, and we 
conjecture that it always terminates on well-typed terms.
% 
Then in \cref{sec:heterogeneous}, we go over two alternative presentations of the 
observational equality: a heterogeneous equality and an equality with no
reduction rules.
% 
In \cref{sec:general-inductives}, we explain how to add non-recursive indexed 
inductive types to the system, and we discuss some difficulties with universe
levels.
% 
Finally, in \cref{sec:proof-rel-impred} we add a universe \( \Prop \) of 
proof-\emph{relevant} impredicative types to \SetoidCC to increase
its computational power.
% 
We show that if we extend the observational equality and the \( \castName \)
operator to this universe, type-checking becomes undecidable.

\section{Cast and Reflexivity}
\label{sec:cast-refl}

\subsection{Observational versus Inductive}

In \SetoidCC, we have a bit of an awkward trade-off between two different 
notions of propositional equality.

On the one hand, we have the observational equality \( \Obseq[A]{t}{u} \),
a proof-irrelevant proposition.
% 
The observational equality has strong selling points: not only does it validate 
the principle of uniqueness of identity proofs by definition (thanks to
computational irrelevance), but it is also equipped with reduction rules that 
bake in useful reasoning principles such as function extensionality and proposition 
extensionality.

But being computationally irrelevant also has its drawbacks: because the equality
proofs carry no information, the eliminator for the observational equality 
has to compute on types constructors instead. This means that \( \castName \) will 
only reduce fully when it is applied to closed types.
% 
To make up for this weakness, \SetoidCC provides the axiom \( \castreflName \) 
which is a proof of observational equality between a cast along reflexivity and the
identity function.
% 
\sideremark{Note that the rule for \( \castreflName \) is phrased with an 
  arbitrary proof of equality between \( A \) and itself. This is equivalent
  to the statement using \( \metaop{refl} \) because of proof irrelevance.}
% 
\begin{mathpar}
	\inferrule[Cast-Refl]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{A}}{\sProp}}
		{\tytmannotated{\Gamma}{\castrefl{A}{t}}{\Obseq[A]{t}{\cast{A}{A}{e}{t}}}{\sProp}}
\end{mathpar}

On the other hand, we have the inductive equality \( \Id{A}{t}{u} \) which is
a proof-relevant inductive type, analogue of the Martin-Löf identity type.
% 
As such, it comes equipped with the usual \( \metaop{J} \) eliminator and its
computation rule. 
% 
\begin{mathpar}
  \inferrule{\tytm{\Gamma}{A}{\Type_i}
			\\ \tytm{\Gamma}{t}{A}
			\\ \tytm{\Gamma}{B}{\Depfun{A}{\Fun{\Id{A}{t}{x}}{\Type_j}}}
			\\ \tytm{\Gamma}{u}{B\ t\ \id{t}}}
			{\red{\Gamma}{\J{A}{t}{B}{u}{t}{\id{t}}}{u}{B\ t\ \id{t}}}
\end{mathpar}

\( \metaop{Id} \) is also stronger than the Martin-Löf identity type since it validates the 
signature reasoning principles of \SetoidCC that are UIP, function 
extensionality and proposition extensionality.
% 
However, computing on reflexivity comes at a price: for the inductive equality,
the principle of UIP only holds up to a propositional equality, and the same 
goes for function extensionality and proposition extensionality.

All in all, the difference is rather subtle. The two relations are logically 
equivalent, but they do not satisfy the exact same computation rules.
% 
As if the distinction between propositional and definitional equality 
wasn't a sufficient source of confusion for newcomers to type theory!

\subsection{The System \SetoidCCplus}

In hope to reconciliate these two propositional equalities, we introduce a 
variation on the observational calculus of constructions, that we call 
\SetoidCCplus. 
% 
This new system supports all the rules of \cref{ch:observational}, plus the rule 
\nameref{inferrule:cast-refl-plus} which promotes the \( \castreflName \) axiom
to a definitional equality.
% 
\begin{mathpar}
	\inferrule[Cast-Refl+]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{A}}{\sProp}}
		{\eqtmannotated{\Gamma}{\cast{A}{A}{e}{t}}{t}{A}{\Type_i}}
  \ilabel{inferrule:cast-refl-plus}
\end{mathpar}

In \SetoidCCplus, we can define a \( J \) eliminator for the observational
equality that computes on reflexivity.
% 
\sideremark{The \( J \) eliminator for the observational equality is defined as \\ \( J(A, t, B, u, t', e) := \) \\
  \( \cast{B\ t\ \metaop{refl}}{B\ t' e}{\eqJ{A}{t}{B}{t'\!}{e}}{u} \)}
% 
To do so, we simply replicate the definition described in 
\cref{sec:inductive-equality}, and in presence of 
rule~\nameref{inferrule:cast-refl-plus} it will actually satisfy the 
computation rule on reflexivity as a definitional equality.
% 
Therefore, the observational equality of \SetoidCCplus really has the best
of both worlds: not only is it definitionally proof irrelevant, but its eliminator 
also supports the computation rule on reflexivity by definition.

Naturally, extending the convertibility relation of \SetoidCC means that we
need a new algorithm to decide conversion for \SetoidCCplus terms.
% 
We will describe two such algorithms below, but unfortunately we are not
able to supplement either with a proof of correction. 
We leave these as conjectures for further work.

\subsection{Implementing \nameref{inferrule:cast-refl-plus} with reduction rules}

The first idea that we might want to try is to add a new reduction rule for 
\( \cast{A}{B}{e}{t} \), so that it will actually reduce to \( t \) in case the
types \( A \) and \( B \) are convertible.
% 
\begin{mathpar}
	\inferrule[Cast-Refl-Red]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
      \tytm{\Gamma}{B}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{B}}{\sProp} \\
      \eqtm{\Gamma}{A}{B}{\Type_i}}
		{\red{\Gamma}{\cast{A}{B}{e}{t}}{t}{A}}
  \ilabel{inferrule:cast-refl-red}
\end{mathpar}

This is a bit careless however, as this rule breaks the determinstic property
of our weak-head reduction strategy.
% because of conflicts with the reduction rules for \( \castName \) on type constructors.
% 
For instance, in the case of a \( \castName \) between two
convertible \( \Pi \)-types, both rule \nameref{inferrule:cast-refl-red} and 
\nameref{inferrule:cast-pi} apply.
% 
\begin{center}
\begin{tikzpicture}
  \node[label=above:{\( \cast{\Depfun{A}{B}}{\Depfun{A}{B}}{\metaop{refl}}{f} \)}] (A) at (0, 0) {} ;
  \node[label=south west:{\( f \)}] (B) at (-2, -0.5) {} ;
  \node[label=below:{\( \begin{array}{l} \lambda\ x\ .\ \metaop{cast}({\subst{B}{a}}, {\subst{B}{a}},
    \\ \phantom{\lambda\ x\ .\ \metaop{cast}}{\metaop{refl}}, {f\ \cast{A}{A}{\metaop{refl}}{x}}) \end{array} \)}]
    (C) at (2, -0.5) {} ;
  \draw [double distance=1.5pt, -{Implies}] (1,0) -- (2,-0.5) ;
  \draw [double distance=1.5pt, -{Implies}] (-1,0) -- (-2,-0.5) ;
\end{tikzpicture}
\end{center}

\sideremark{In fact, the reader can check that all critical pairs introduced by
rule \nameref{inferrule:cast-refl-red} can be unified by reducing under binders.
Therefore, this rule is more reasonable in the context of a deep reduction 
strategy.}
% 
Note that we need to reduce under the \( \lambda \)-abstraction to unify this
critical pair, but reducing under binders is not allowed in the weak-head 
reduction strategy. 
% 
A non-deterministic \emph{and} non-confluent reduction strategy is no good,
so rule~\nameref{inferrule:cast-refl-red} is not a reasonable addition to
the weak-head reduction.

To avoid such conflicts with the reduction rules on constructors, we can 
restrict our rule to apply only when \( A \) and \( B \) are neutral types.
% 
This way, we recover a deterministic reduction strategy, although we lose 
stability of weak-head reduction under substitution.
% 
\begin{mathpar}
	\inferrule[Cast-Refl-Neutral]
		{%\tytm{\Gamma}{A}{\Univ_i}
			\tytmannotated{\Gamma}{t}{A}{\Type_i} \\
      \tytm{\Gamma}{B}{\Type_i} \\
			\tytmannotated{\Gamma}{e}{\Obseq[{\Type_i}]{A}{B}}{\sProp} \\
      \eqtm{\Gamma}{A}{B}{\Type_i} \\
      \text{\( A \) and \( B \) neutral}}
		{\red{\Gamma}{\cast{A}{B}{e}{t}}{t}{A}}
  \ilabel{inferrule:cast-refl-neutral}
\end{mathpar}

Still, rule~\nameref{inferrule:cast-refl-neutral} is by no 
means innocuous, as it dramatically changes the behavior of the system.
% 
Unlike all the other reduction rules of \SetoidCCplus, it is not triggered by 
a redex consisting of a destructor applied to a constructor, but rather by a 
convertibility premise.
% 
As a consequence, it deeply intertwines weak-head reduction with conversion
checking: when computing the head normal form of a term, it might be necessary
to check convertibility of two terms, which requires putting them in weak head 
normal form, etc.

\paragraph*{Normalization}

Type casting operators that reduce on identical types have a long history in
type theory. 
% 
In the context of impredicative systems, they were notably studied by Girard 
for System F \sidecite{girard72}, and by Werner for \CIC \sidecite{Werner2008}.
% 
In both cases, it unfortunately turns out that the resulting system loses 
normalization for open terms \sidecite{lmcs:6606}.

This incompatibility between impredicativity and type casting can be 
interpreted as a consequence of the breakage of parametricity.
%
Indeed, the operator \( {\castName : (A\ B\ : \Type_i) \to A \to B} \) 
is fundamentally anti-parametric because in case \( A \equiv B \) it
reduces to \( t \), which satisfies the parametricity predicate associated 
to \( A \) but not necessarily the predicate associated to \( B \).
% 
But normalization models for impredicative type theories are usually built
using reducibility candidates, which rely crucially on parametricity to 
work.

Fortunately, we expect that our system \SetoidCCplus will avoid these issues, 
because \( \castName \) only computes in the predicative fragment and 
proof-irrelevance once again comes to our rescue in the impredicative 
layer.
% 
Therefore, we feel justified in conjecturing that this system is in fact 
normalizing.
% 
\begin{conjecture}
  \SetoidCCplus with computation rules for \( \castName \) on neutral types is 
  normalizing.
\end{conjecture}

As further evidence for this conjecture, we remark that the situation is very 
similar to reduction rules for \( \castName \) on type constructors, 
which are certainly not parametric either.
% 
In fact, in \cref{sec:proof-rel-impred} we will show that extending these rules 
to a proof-relevant impredicative universe breaks normalization for the exact 
same reason.
% 
Nonetheless, by restricting computation to the predicative layer, we were able 
to prove normalization for \SetoidCC in \cref{ch:metatheory}. 

\subsection{Implementing \nameref{inferrule:cast-refl-plus} in conversion checking}

In conclusion to the previous section, our attempt at implementing 
rule~\nameref{inferrule:cast-refl-plus} with reduction is not a resounding 
success.
% 
The resulting reduction strategy cannot be defined independently from 
conversion checking, and it is not even stable under substitution.
% 
At that point, it is not clear that we gain much by performing said reductions.

Here, it is tempting to draw a parallel with the \( \eta \)-equality of functions. 
% 
Just like rule~\nameref{inferrule:cast-refl-plus}, the \( \eta \)-equality rule 
does not fit the general pattern of reducing a redex, and it does not lend 
itself too well to implementation \textit{via} reduction rules either---both the
\( \eta \)-expansion and the \( \eta \)-reduction ideas are unsatisfying
\sidecite{meven_eta}.
% 
\sideremark{This is also how our conversion checking algorithm from 
\cref{ch:metatheory} proceeds.}
In practice, all major proof assistants chose to implement \( \eta \) during 
the conversion checking phase: when the algorithm needs to decide 
convertibility between a function and a neutral term, it performs an \( \eta \)
expansion of the neutral term and calls itself recursively.

Therefore we suggest that it might be wise to imitate this strategy for 
\nameref{inferrule:cast-refl-plus}, \ie to give up on an implementation 
\textit{via} reduction rules and to handle that equality during conversion 
checking instead, as described in the work of Allais \etal~\sidecite{allais13}.
% 
Concretely, we propose the following informal algorithm for conversion 
checking in \SetoidCCplus:
% 
\begin{figure}[!h]
\[
\begin{array}{l}
\textbf{Input:} \enskip \text{A context \( \Gamma \), three terms \( A \), \( t \), \( u \), and an integer \( i \).} \\
\textbf{Output:} \enskip \text{`yes' if \( \eqtmannotated{\Gamma}{t}{u}{A}{\Type_i} \), `no' otherwise.} \\
\textbf{Begin} \\
\quad A \longleftarrow \text{the weak head normal form of \( A \)} \\
\quad t \longleftarrow \text{the weak head normal form of \( t \)} \\
\quad u \longleftarrow \text{the weak head normal form of \( u \)} \\
\quad \textbf{if} \enskip \text{\( A \) starts with a type constructor} \enskip \textbf{then} \\
\quad \quad \text{proceed as usual} \\
\quad \textbf{else} \\
\quad \quad \textbf{if} \enskip t = \cast{B}{C}{e}{t'} \enskip \textbf{and} \enskip \text{\( u \) does not start with \( \castName \)} \enskip \textbf{then}\\
\quad \quad \quad \text{do a recursive call to decide whether \( \eqtm{\Gamma}{B}{C}{\Type_i} \)} \\
\quad \quad \quad \textbf{if} \enskip \text{`yes'} \enskip \textbf{then} \\
\quad \quad \quad \quad \text{do a recursive call on \( t' \) and \( u \)} \\
\quad \quad \quad \textbf{if} \enskip \text{`no'} \enskip \textbf{then} \\
\quad \quad \quad \quad \textbf{return} \enskip \text{`no'} \\
\quad \quad \textbf{else if} \enskip \text{\( t \) does not start with \( \castName \)} \enskip \textbf{and} \enskip u = \cast{B}{C}{e}{u'} \enskip \textbf{then} \\
\quad \quad \quad \text{do a recursive call on \( B \) and \( C \), \textit{etc}, as above} \\
\quad \quad \textbf{else} \\
\quad \quad \quad \text{proceed as usual}
\end{array}
\]
\caption{Conversion checking algorithm for \SetoidCCplus}
\end{figure}

\begin{conjecture}
  The algorithm sketched above decides conversion for \SetoidCCplus.
\end{conjecture}

We leave the investigation of this question for future works.

\section{Variations on the Observational Equality}
\label{sec:heterogeneous}

\subsection{A Proof-Irrelevant Heterogeneous Equality}

In \SetoidCC, we can form the observational 
equality of two terms only when they have the same type. 
% 
In other words the observational equality is homogeneous---just like the 
definitional equality, the inductive equality, or most equalities routinely 
considered by type theorists.
% 
But when they introduced observational type theory, Altenkirch, McBride and 
Swierstra decided to use a heterogenous 
equality~\sidecite{altenkirchAl:plpv2007}, arguing that it results in simpler
rules.
% 
In this section, we show that it is not too difficult to modify the 
presentation of \SetoidCC so that it uses a proof-irrelevant heterogeneous 
equality instead of the homogeneous observational equality.

% It is not too difficult to modify the presentation of \SetoidCC so that it uses a
% proof-irrelevant heterogeneous equality \textit{à la} Altenkirch \etal instead 
% of the homogeneous observational equality.
% 
First, we change the formation rule of the equality so that it applies between
two terms with different types:
% 
\begin{mathpar}
  \inferrule[Heterogenous-Eq-Form]
    {\tytmannotated{\Gamma}{t}{A}{\Type_i}
    \\ \tytmannotated{\Gamma}{u}{B}{\Type_i}}
    {\tytm{\Gamma}{\Heteq{t}{A}{u}{B}}{\sProp}}
  \and
\end{mathpar}

Note that unlike the ``path-over'' equality of cubical type theory, we do not 
ask for a proof of equality between \( A \) and \( B \) to form the type
\( \Heteq{t}{A}{u}{B} \).

We also replace rules \nameref{inferrule:refl},
\nameref{inferrule:transport-prop} and \nameref{inferrule:cast} with the 
following:
\begin{mathpar}
  \inferrule[Heterogeneous-Transport-\( \sProp \)]
		{%\tytm{\Gamma}{A}{\Type_i}
    \tytmannotated{\Gamma}{t}{A}{\Type_i}
    \\ \tytm{\extctx[x]{\extctx[X]{\Gamma}{\Type_i}}{X}}{B}{\sProp} 
    \\ \tytmannotated{\Gamma}{u}{\subst[x]{\subst[X]{B}{A}}{t}}{\sProp}
    \\ \tytmannotated{\Gamma}{t'}{A'}{\Type_i}
    \\ \tytmannotated{\Gamma}{e}{\Heteq{t}{A}{t'}{A'}}{\sProp}}
  {\tytmannotated{\Gamma}{\hettransport{A}{t}{B}{u}{A'}{t'}{e}}{\subst[x]{\subst[X]{B}{A'}}{t'}}{\sProp}}
\end{mathpar}
\begin{mathpar}
	\inferrule[Heterogeneous-Cast]
		{% \tytm{\Gamma}{A}{\Univ_i}   \quad \tytm{\Gamma}{B}{\Univ_i}
			\tytmannotated{\Gamma}{e}{\Heteq{A}{\Univ}{B}{\Univ}}{\sProp}
			\quad \tytmannotated{\Gamma}{t}{A}{\Univ}}
		{\tytmannotated{\Gamma}{\cast{A}{B}{e}{t}}{B}{\Univ}}
  \and
  \inferrule[Heterogeneous-Refl]
    {\tytmannotated{\Gamma}{t}{A}{\Type_i}}
    {\tytmannotated{\Gamma}{\refl{t}{t}}{\Heteq{t}{A}{t}{A}}{\sProp}}
\end{mathpar}

From a computational perspective, the heterogeneous equality does not play
the role of a type former, but rather that of an eliminator of the 
universe---just like the homogeneous observational equality.
% 
In other words, the term \( \Heteq{t}{A}{u}{B} \) computes by comparing the 
head constructors of the types \( A \) and \( B \).
% 
If the constructors happen to match, \( \Heteq{t}{A}{u}{B} \) reduces to the
definition of the observational equality for the corresponding base type,
% 
and if \( A \) and \( B \) have incompatible head constructors, then 
\( \Heteq{t}{A}{u}{B} \) reduces to \( \Empty \)
\sideremark[*-3]{Reducing to \( \Empty \) on non-matching types is optional. It is 
  a reasonable axiom, but it also adds an unnecessary constraint on the models 
  of the theory.}.

Naturally, we also adjust the rules that describe equality on the base types. 
For instance, the heterogeneous equality between two dependent pairs now reads 
as follows:
% 
\begin{mathpar}
  \inferrule[Heterogeneous-Eq-Pair]{\tytmannotated{\Gamma}{t,u}{\Depsumannotated{A}{B}{}{\Type_i}{\Type_j}}{\Type_{\mathrm{max}(i,j)}}} 
			{\redmultiline{\Gamma}{\Heteq{t}{\Sigma A B}{u}{\Sigma A B}}{(\Heteq{\relfst{t}}{A}{\relfst{u}}{A})
				\land {(\Heteq{\relsnd{t}}{B[\relfst{t}]}{\relsnd{u}}{B[\relfst{u}]})}}{\sProp}}
\end{mathpar}

Remark that contrary to rule \nameref{inferrule:eq-pair}, we do not need to use
type casting to state equality between the second projections. 
% 
This is exactly what makes the heterogeneous equality nicer than the homogeneous 
equality: it can be defined independently from \( \castName \).

Other than making the system more modular, replacing the observational equality with the 
heterogeneous variant does not change the behavior of \SetoidCC too much. 
% 
It seems safe to assume that all the meta-theory that we developed in
\cref{ch:metatheory} would work just as well with a heterogeneous equality.
Thus the choice between homogeneous and heterogeneous is a matter of taste
more than anything.

\subsection{Equality Without Computation}

In fact, the exact nature of the observational equality does not really matter.
% 
\sideremark{Two types are incompatible if they start with different head constructors. 
  Since \( \castName \) between incompatible types are stuck terms, we want
  to make sure they can't be formed in the empty context, lest we get 
  non-canonical integers.}
% 
As a computationally irrelevant property, its sole purpose is to add a 
logical constraint to \( \castName \) so that it becomes impossible to cast 
between two incompatible types in an empty context.

Therefore, any alternative definition fine as long as it does not add 
equalities between incompatible types in an empty context, and as long as it allows us to
give a type to all the reduction rules for \( \castName \). 
% 
For instance, a \( \castName \) between two dependent sums reduces to a pair of 
\( \castName \) (rule~\nameref{inferrule:cast-sigma}), which means that in 
particular, we need to derive a proof of \( \Obseq[\Type]{A}{A'} \) from a proof of 
\( \Obseq[\Type]{\Depsum{A}{B}}{\Depsum{A'}{B'}} \).

But there is really nothing that requires these derivations to come from 
reduction rules. In fact, we can completely do away with the computation rules
for the observational equality, and replace them with proof-irrelevant axioms
-- for instance the following two axioms for equality on dependent sums:
\[
\begin{array}{lcl}
\metaop{eq{-}fst} & : & \Obseq[\Type]{\Depsum{A}{B}}{\Depsum{A'}{B'}} \enskip \to \enskip \Obseq[\Type]{A}{A'} \\
\metaop{eq{-}snd} & : & \Pi (e : \Obseq[\Type]{\Depsum{A}{B}}{\Depsum{A'}{B'}}) \enskip . \\
& & \enskip \Pi (a : A)\ .\ \Obseq[\Type]{\subst{B}{a}}{\subst{B'}{\cast{A}{A'}{\metaop{eq{-}fst}\ e}{a}}}.
\end{array}
\]

And the resulting system still enjoys pretty much all the properties of 
\SetoidCC, except that the proof of function extensionality is not given by 
reflexivity anymore.
% 
This variation on observationaly type theory has been explored and implemented by 
Atkey~\sidecite{AtkeySOTT}, who argues that this avoids some problems with 
explosion of the size of the types.

\section{Non-Recursive Indexed Inductive Types}
\label{sec:general-inductives}

In our presentation of the system, we described the type of natural numbers, 
% 
\sideremark{Technically speaking, the dependent sums of \SetoidCC are 
  \emph{negative}, meaning they are defined with projections and an eta-equality 
  rule, not as an inductive type with a single constructor.
  Close enough.}
% 
the dependent sums and the inductive equality which are three examples of
inductive types.
% 
Together, they cover the three fundamental features of inductive types: 
recursion, type dependency, and indices. 
% 
But in practice, proof assistant users are understandably not too fond
of encoding their inductive definitions with these three basic types---the 
standard approach is rather to have a general scheme for inductive 
definitions.
% \sideremark{It is actually not even possible to encode W-types with 
%   \( \Nat \), \( \Sigma \) and \( \metaop{Id} \).}

In this section, we discuss the extension \SetoidCC with a scheme for 
non-recursive indexed inductive types.
% 
Now, inductive schemes have this unfortunate tendency to unleash 
notation hell, with vectors of parameters, indices and constructors. 
% 
We will not be able to escape this completely, but we still make some 
simplifying assumptions:
% 
no mutual definitions, only one index in the type signature and only one 
argument per constructor. Concretely, a generic definition might look like this:\\
\sideremark{Note that it is possible to use dependent sums to pack multiple 
  indices or parameters in this definition, and thus these restrictions are
  not too serious.}
\[
\begin{array}{l}
\mathsf{Inductive}\ I : A \to \Type_j := \\
\sep \quad c_0 \enskip : \enskip \Depfun{B_0}{I\ t_0}\\
\sep \quad ... \\
\sep \quad c_n \enskip : \enskip \Depfun{B_n}{I\ t_n}
\end{array}
\]

This definition is well-formed if 
\sideremark{The constraint \( i \le j \) is not required in proof assistats 
  such as \Coq or \Agda. But in an observational type theory, it is essential
  to prevent inconsistencies---see \cref{sec:univ-levels}.}
\begin{itemize}
  \item \( A \) is a closed type of sort \( \Type_i \), and \( i \le j \)
  \item for all \( k \) the term \( B_k \) is a closed type of sort \( \Univ_k \) 
  such that \( \Univ_k \) is either \( \sProp \) or a universe bounded by \( \Type_j \), and
  \item for all \( k \) the term \( t_k \) is a term of type \( A \) in context \( B_k \).
\end{itemize}

The elimination rule generated by this definition is the following:
\begin{mathpar}
\inferrule[Ind-elim]
  {\tytm{\Gamma}{P}{\Depfun{A}{I\ x \to \Univ}}
  \\ \{ \enskip \tytm{\Gamma}{p_k}{\Depfun{B_k}{P\ t_k\ (c_k\ x)}} \enskip \}_{1 \le k \le n}
  \\ \tytmannotated{\Gamma}{a}{A}{\Type_i}
  \\ \tytmannotated{\Gamma}{u}{I\ a}{\Type_j}}
  {\tytmannotated{\Gamma}{\metaop{Ind{-}elim}(P, p_1, ..., p_n, a, u)}{P\ a\ u}{\Univ}}
\and
\inferrule[Ind-comp]
  {[...]}
  {\redannotated{\Gamma}{\metaop{Ind{-}elim}(P, p_1, ..., p_n, a, c_k\ b)}{p_k\ b}{P\ \subst{t_k}{b}\ (c_k\ b)}{\Univ}}
\end{mathpar}
and the observational equality between two instances of \( I \) simply reduces 
to the equality between the indices.
\[
\inferrule[Ind-eq]
  {\tytmannotated{\Gamma}{a}{A}{\Type_i}
  \\ \tytmannotated{\Gamma}{a'}{A}{\Type_i}}
  {\red{\Gamma}{\Obseq[\Type_j]{I\ a}{I\ a'}}{\Obseq[A]{a}{a'}}{\sProp}}
\]
Equality between two constructors is also straightforward: if the constructors
match, it reduces to the equality of the arguments, and it reduces to \( \Empty \)
otherwise.
\begin{mathpar}
\inferrule[Ind-Cons-eq]{\tytmannotated{\Gamma}{b}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{b'}{B_k}{\Univ_k}}
  {\red{\Gamma}{\Obseq[I\ \_]{c_k\ b}{c_k\ b'}}{\Obseq[B_k]{b}{b'}}{\sProp}}
\and
\inferrule[Ind-Cons-neq]
  {\tytmannotated{\Gamma}{b}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{b'}{B_\ell}{\Univ_\ell}}
  {\red{\Gamma}{\Obseq[I\ \_]{c_k\ b}{c_\ell\ b'}}{\Empty}{\sProp}}
  {k \neq \ell}
\end{mathpar}

As with the \( \metaop{Id} \) types from \cref{sec:inductive-equality}, applying 
\( \castName \) to a canonical inhabitant of \( I\ a \) might produce a non-canonical 
term. For instance, consider the following term in a context with
\( e : \Obseq{a}{a'} \).
\[
\cast{I\ a}{I\ a'}{e}{c_0\ b}
\]
We have no way to reduce it to an element of \( I\ a' \) of the form 
\( c_0\ b' \), because such a term would only be well-typed if 
\( \subst{t_0}{b'} \) is convertible to \( a' \).
% 
As we have no way to satisfy this constraint, we add \( n \) new canonical terms
\( c_0^\sim , ... , c_n^\sim \) that relax the convertibility constraint to an 
observational equality:
\[
\inferrule{\tytmannotated{\Gamma}{a}{A}{\Type_i}
  \\ \tytmannotated{\Gamma}{b}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{e}{\Obseq[A]{\subst{t_k}{b}}{a}}{\sProp}}
  {\tytmannotated{\Gamma}{c_k^\sim(b,e)}{I\ a}{\Type_j}}
\]
The observational equality is defined on these new terms just like it is 
defined on the regular constructors:
% 
\sideremark[*0]{This rule has also a symmetric counterpart}
% 
\begin{mathpar}
\inferrule{\tytmannotated{\Gamma}{b, b'}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{e}{\Obseq[A]{\subst{t_k}{b'}}{\subst{t_k}{b}}}{\sProp}}
  {\red{\Gamma}{\Obseq[I\ \_]{c_k\ b}{c_k^\sim(b',e)}}{\Obseq[B_k]{b}{b'}}{\sProp}}
\and
\inferrule{\tytmannotated{\Gamma}{a}{A}{\Type_i}
  \\ \tytmannotated{\Gamma}{b, b'}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{e}{\Obseq[A]{\subst{t_k}{b}}{a}}{\sProp}
  \\ \tytmannotated{\Gamma}{e}{\Obseq[A]{\subst{t_k}{b'}}{a}}{\sProp}}
  {\red{\Gamma}{\Obseq[I\ a]{c_k^\sim(b, e)}{c_k^\sim(b',e')}}{\Obseq[B_k]{b}{b'}}{\sProp}}
\end{mathpar}

The eliminator of \( I \) computes on the new constructors \emph{via} \( \metaop{cast} \).
\begin{mathpar}
\inferrule
  {\tytm{\Gamma}{P}{\Depfun{A}{I\ x \to \Univ}}
  \\ \{ \enskip \tytm{\Gamma}{p_k}{\Depfun{B_k}{P\ t_k\ (c_k\ x)}} \enskip \}_{1 \le k \le n}
  \\ \tytmannotated{\Gamma}{a}{A}{\Type_i}
  \\ \tytm{\Gamma}{e}{\Obseq[A]{\subst{t_k}{b}}{a}}
  \\ e' := \ap{P}{e}\ b}
  {\redmultiline{\Gamma}{\metaop{Ind{-}elim}(P, p_1, ..., p_n, a, c_k^\sim(b,e))}{\enskip \cast{P\ \subst{t_k}{b}\ (c_k\ b)}{P\ a\ (c_k^\sim(b,e))}{e'}{p_k\ b}}{P\ a\ (c_k^\sim(b,e))}{{} : \Univ}}
\end{mathpar}

And finally, the computation rules for \( \metaop{cast} \) are straightforward to 
define---they accumulate the equalities in the second argument of \( c_k^\sim \):
\begin{mathpar}
\inferrule{\tytmannotated{\Gamma}{a}{A}{\Type_i}
  \\ \tytmannotated{\Gamma}{b}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{e}{\Obseq[A]{\subst{t_k}{b}}{a}}{\sProp}}
  {\redannotated{\Gamma}{\cast{I\ \subst{t_k}{b}}{I\ a}{e}{c_k\ b}}{c_k^\sim(b,e)}{I\ a}{\Type_j}}
\end{mathpar}
\begin{mathpar}
\inferrule{\tytmannotated{\Gamma}{a, a'}{A}{\Type_i}
  \\ \tytmannotated{\Gamma}{e}{\Obseq[A]{a}{a'}}{\sProp}
  \\ \tytmannotated{\Gamma}{b}{B_k}{\Univ_k}
  \\ \tytmannotated{\Gamma}{e'}{\Obseq[A]{\subst{t_k}{b}}{a}}{\sProp}}
  {\redannotated{\Gamma}{\cast{I\ a}{I\ a'}{e}{c_k^\sim(b,e')}}{c_k^\sim(b,e \cdot e')}{I\ a'}{\Type_j}}
\end{mathpar}

All in all, these reduction rules are a bit of a eyesore, but the underlying 
idea is just to use the same trick as in our definition of the inductive 
equality.
% 
I expect that adding recursion to our inductive scheme does not pose 
fundamental problems, but the rules become much more complex.

\subsection{Indices and Universe Levels}
\label{sec:univ-levels}

As we see, devising sensible computation rules for indexed inductive types
is not too difficult.
% 
However, we still need to pay close attention to the consistency of our 
system when adding indexed inductive types:
consider the following type with one index and no constructors:
\[
\begin{array}{l}
\mathsf{Inductive}\ \mathrm{Emb} : (\Type_0 \to \Type_0) \to \Type_0 := \emptyset \\
\end{array}
\]

In proof assistants based on intensional type theory such as \Coq or \Agda, 
this definition is accepted even though the index type is larger than 
\( \Type_0 \). 
Of course this is safe and sound, as evidenced by the set-theoretic model of 
\CIC~\sidecite{pcuic}, but the situation is very different in \SetoidCC.

Remember that we defined the following computation rule for the observational 
equality:
\[
\inferrule[Ind-eq]
  {\tytm{\Gamma}{X}{\Type_0 \to \Type_0}
  \\ \tytm{\Gamma}{Y}{\Type_0 \to \Type_0}}
  {\red{\Gamma}{\Obseq[\Type_0]{\mathrm{Emb}\ X}{\mathrm{Emb}\ Y}}{\Obseq[\Type_0 \to \Type_0]{X}{Y}}{\sProp}}
\]
This means that Emb is an injection of \( \Type_0 \to \Type_0 \) into \( \Type_0 \), 
which is obviously anti-classical. 
In fact the impredicativity of \( \sProp \) is enough to derive a contradiction,
with an argument due to Hur~\sidecite{chungkil}: we start by defining
\[
\begin{array}{lcl}
P & : & \Type_0 \to \Type_0 \\
P & := & \lambda\ (x : \Type_0)\ .\ \Box \varExists[a]{\Type_0 \to \Type_0}{((\Obseq{\mathrm{Emb}\ a}{x}) \land (a\ x\ \to \Empty))} \\
\end{array}
\]
And then we show the following three theorems, which encode Russell's 
antinomy.
\[
\begin{array}{lcl}
negP & : & P\ (\mathrm{Emb}\ P) \to \Empty \\
negP\ H_P & := & \metaop{transp}({\Type_0 \to \Type_0},\ {\fst{\unboxt{H_P}}}, \\
& & \phantom{\metaop{transp}(}{\lambda\ X\ .\ X\ (\mathrm{Emb}\ P) \to \Empty}, \\
& & \phantom{\metaop{transp}(}{\snd{\snd{\unboxt{H_P}}}},\ {P},\ {\fst{\snd{\unboxt{H_P}}}})
\\
\\
posP & : & (P\ (\mathrm{Emb}\ P) \to \Empty) \to P\ (\mathrm{Emb}\ P) \\
posP\ H_P & := & \boxt{\pair{P}{\pair{\refl{P}{P}}{H_P}}}
\\
\\
abs & : & \Empty \\
abs & := & negP\ (posP\ negP)
\end{array}
\]
Thus we see that it is necessary to have a constraint on the universe
level of indices in inductive types, because of rule \textsc{Ind-eq}.
Of course, the same argument applies to parameters as well.

\section{Proof-Relevant Impredicativity}
\label{sec:proof-rel-impred}

\subsection{\SetoidCC versus \CIC}

Our system is named the ``Observational Calculus of Constructions''
to highlight the similarities with the Predicative Calculus of Inductive 
Constructions, the type theory which serves as the base of the \Coq proof 
assistant \sidecite{pcuic}.
% 
Most notably, both systems enforce a separation between an impredicative 
universe of propositions and a hierarchy of predicative universes, with the
idea that the predicative layer cannot tell apart two proofs of a same 
proposition.

% Despite this superficial resemblance, the two systems do not 
% As we explained in \cref{sec:analys-norm-proof}, the 

But there is an important difference in the treatment of impredicativity: 
% 
\sideremark{Mind the different notation: \( \sProp \) is the proof-\emph{irrelevant}
universe of propositions from \SetoidCC, and \( \varProp \) is the proof-\emph{relevant}
universe of propositions from \CIC.}
% 
contrary to \SetoidCC, the impredicative universe \( \varProp \) of \CIC is 
\emph{computationally relevant}. By this, we mean that there is no 
analogue of the rule \nameref{inferrule:proof-irrelevance}.
% 
Therefore, in \CIC proofs of propositions are subject to computation rules, just 
like the terms that inhabit the predicative sorts; and the normalization theorem 
applies to both layers.

Furthermore, \CIC allows inductive definitions in the universe of propositions \( \varProp \).
% 
Just like regular inductive types, an inductive proposition may have parameters, 
indices, many constructors with arguments, \etc
% 
but its eliminator is restricted to propositional predicates in order 
to avoid paradoxes. For instance, here is what an inductive definition of the 
\emph{propositional} integers might look like in \CIC:
\[
\begin{array}{l}
\mathsf{Inductive}\ \Natprop : \varProp := \\
| \enskip \zero : \Natprop \\
| \enskip \sucName : \Natprop \to \Natprop
\end{array}
\]

They work just like the usual integers, except that they cannot be eliminated
into a type that is not a proposition.

However, there is an exception to this elimination constraint: if the inductive proposition is a 
\emph{subsingleton}, \ie it has at most one constructor and all constructor 
arguments are propositions, then
% 
\defnote[*-4]{large elimination}{An inductive type is said to support large 
elimination if its eliminator can be used with arbitrary predicates.}
% 
is allowed.
% 
Large elimination of subsingletons is very important, as it plays the role of 
a bridge that connects the two layers of \CIC, and allows some of the raw
logical power of impredicativity to leak to the predicative layer.
% 
More concretely, we can use impredicativity and large elimination to define 
all the integer functions that are provably computable in higher-order Peano
Arithmetic as terms of type \( \Nat \to \Nat \) in the predicative layer 
(and then some more!).

Compare this situation to \SetoidCC, where most propositions are defined
using impredicative encodings.
Since impredicative encodings do not provide large elimination principles, 
the only propositions that may be eliminated into the predicative layer are 
the false proposition (\textit{via} \( \emptyrecName \)) and the observational 
equality (\textit{via} \( \castName \)).
% 
\sideremark{The observational equality is technically not an inductive proposition, but its
  role is similar enough to the inductive equality that it makes sense to compare
  \( \castName \) to a large elimination principle.}
% 
This impoverished communication between the two layers confines the power of 
impredicativity to the proof-irrelevant layer: 
while it may be possible to define very complex functions \textit{via}
an impredicative encoding of their graph, there is no way to apply them to
proof-relevant integers.
% 
As a result, \SetoidCC cannot define more functions in the proof-relevant
layer than \MLTT (see \cref{sec:analys-norm-proof}), 
despite being an impredicative type theory.

This whole story makes a rather compelling argument for the addition of
inductive propositions with subsingleton elimination to \SetoidCC.
% 
Unfortunately, proof irrelevance is a serious obstacle:
% 
how is the eliminator supposed to compute when we destroyed all computational
information from the proofs? 
% 
In the case of the observational equality we can get by with computation on 
types, but there is no way this trick generalizes to all subsingleton 
propositions. 
% 
A more in-depth discussion of the difficulties with large elimination of
a proof-irrelevant accessibility predicate can be found in the work of Gilbert
\etal \sidecite{gilbert:hal-01859964}.

% \subsection{The Accessibility Predicate}

% Examples of subsingleton inductive propositions include the false proposition,
% the inductive equality, and the accessibility predicate.
% \[
% \begin{array}{l}
% \mathsf{Inductive}\ \Acc\ (A : \Type)\ (R : A \to A \to \sProp) : A \to \sProp := \\
% | \enskip \accintro : \Depfun[x]{A}{(\Depfun[y]{A}{(R\ y\ x \to \Acc\ A\ R\ y)})} \to \Acc\ A\ R\ x
% \end{array}
% \]

% Given a type \( A \) and a propositional relation \( R \) on \( A \), the 
% accessibility predicate for \( R \) characterizes those elements of \( A \) 
% whose tree of predecessors for \( R \) is well-founded.
% % 
% The accessibility predicate is a rather advanced tool, but its large 
% elimination principle plays an important role in the extraction of algorithms 
% from functions defined \textit{via} an impredicative encoding of their graph 
% \sidecite{bragamethod}.

% \[
% \begin{array}{l}
% \Acc{-}\mathsf{elim}\ \{A : \mathsf{Type}\}\ \{R : A \to A \to \sProp\}\ (P : A \to \Univ) :
% \\ \hspace{2em}((x : A) \to
% \\ \hspace{4em} ((y : A) \to R\ y\ x \to \Acc\ R\ y) \to
% \\ \hspace{4em} ((y : A) \to R\ y\ x \to P\ y) \to P\ x) \to
% \\ \hspace{2em} (x : A) \to \Acc\ R\ x \to P\ x
% \end{array}
% \]

\subsection{Extending \SetoidCC with the Power of Proof-Relevant Impredicativity}

Since \( \sProp \) cannot support subsingleton elimination, it seems that the 
only way to recover the full power of impredicativity in \SetoidCC is to extend 
% 
the system with an impredicative universe of proof-\emph{relevant} propositions 
\( \Prop \). 
% 
\sideremark{We use \( \varProp \) for the universe of propositions in \CIC, and
\( \Prop \) for the universe of proof-relevant propositions in \SetoidCC.}
% 
Now this new universe cannot replace \( \sProp \), because 
proof-irrelevant axioms play an important role in the definition of
the observational equality. 
% 
Therefore, in this section we study an extension of \SetoidCC that supports both \( \sProp \) 
and \( \Prop \) in the same system.
% 
\begin{mathpar}
    \inferrule[Univ-Prop]
      {\wfctx{\Gamma}}
      {\tytm{\Gamma}{\sProp}{\Type_0}}
  \and
    \inferrule[Univ-Prop-Rel]
      {\wfctx{\Gamma}}
      {\tytm{\Gamma}{\Prop}{\Type_0}}
\end{mathpar}

Having two different universes of propositions is a bit confusing! But their
roles do not overlap that much: while \( \sProp \) is used to enforce 
proof-irrelevant logical constraint on types (including the observational 
equality), \( \Prop \) is only here as a tool for impredicative definitions.
% 
Since \( \Prop \) is an inhabitant of the universe \( \Type_0 \), it means we can form
observational equalities between inhabitants of \( \Prop \).
% 
How should they compute?

A first idea is to mimic the rule for \( \sProp \) and define the 
observational equality as logical equivalence. This way, we get propositional
extensionality for the proof-relevant propositions.
% 
\sideremark{Since the observational equality is a proof-irrelevant proposition,
we need the squash to convert that equivalence to an element of
\( \sProp \).}
% 
\begin{mathpar}
	\inferrule[Eq-$\Prop$]
		{\tytm{\Gamma}{A}{\Prop}
			\\ \tytm{\Gamma}{B}{\Prop}}
		{\red{\Gamma}{\Obseq[{\Prop}]{A}{B}}{\Squash{\Prod{(\Fun{A}{B})}{(\Fun{B}{A})}}}{\sProp}}
	\ilabel{inferrule:eq-sprop}
\end{mathpar}

Unfortunately, this definition is problematic from a computational perspective:
% 
if  \( t \) is an inhabitant of some proof-relevant proposition \( A \), then 
we should be able to obtain an inhabitant of \( B \) from a proof of 
\( \Obseq[\Prop]{A}{B} \). 
% 
But since the proof of equality is a \emph{squashed} equivalence, we have no
way to use it to compute an element of \( B \).

A second idea is to mimic the observational equality on the predicative 
universes, \ie to pick reduction rules according to the head constructors of the
propositions. 
% 
Then, we extend the \( \castName \) operator to work with proof-relevant propositions, 
too
\begin{mathpar}
	\inferrule[Cast-\( \Prop \)]
		{% \tytm{\Gamma}{A}{\Univ_i}   \quad \tytm{\Gamma}{B}{\Univ_i}
			\tytmannotated{\Gamma}{e}{\Obseq[{\Prop}]{A}{B}}{\sProp}
			\quad \tytmannotated{\Gamma}{t}{A}{\Prop}}
		{\tytmannotated{\Gamma}{\cast{A}{B}{e}{t}}{B}{\Prop}}
\end{mathpar}
% 
along with reduction rules for \( \cast{A}{B}{e}{t} \) when \( A \) and \( B \) have
matching head constructors, just like the predicative \( \castName \).
This idea seems more reasonable, but it turns out to have the very unfortunate
consequence of making conversion checking undecidable.

\begin{theorem}
  If we extend \( \castName \) to the universe of proof-relevant impredicative
  propositions \( \Prop \) that supports inductive definitions, then conversion 
  becomes undecidable.
\end{theorem}

The proof of this fact will occupy us for the remainder of this section.

\paragraph{Outline of the Proof of Undecidability}
% 
Our proof is based on the argument of Abel and Coquand \sidecite{lmcs:6606},
that we enhance to a proof of undecidability.
% 
The main idea is to use impredicativity and \( \castName \) to 
approximate a fixed-point combinator.

Given a proof-relevant proposition \( A : \Prop \) and a function
\( f : A \to A \), impredicativity makes it possible to give a type to the 
term
\[
\Delta_f := \lambda x .\ f\ (x\ x)
\]
which is one half of the fixed point combinator \( Y_f := \Delta_f\ \Delta_f \).
%
We cannot complete the combinator though, as the type of \( \Delta_f \) does 
not allow self-application.
% 
But in an inconsistent context, \( \castName \) can be used to convert between 
any two types whatsoever.
% 
Thus we can apply \( \Delta_f \) to a type-cast of \( \Delta_f \), which 
results in a term that is somewhat similar to a fixed-point combinator.

Then, using this pseudo-fixed point combinator, we can build a term that 
loops through the iterates of any integer function:
%
\sideremark{Recall that \( \Natprop \) is the inductive type of 
  natural numbers in \( \Prop \).}
% 
\begin{lemma}
  Let \( g \) be a closed term of type \( \Fun{\Natprop}{\Natprop} \).

  In an inconsistent context, we can form a term \( \mathrm{dec}_g \)
  of type \( \Natprop \) which is convertible to 0 if there is 
  a positive integer \( n \) such that \( g^n\ 1 \equiv 0 \), and diverges otherwise.
\end{lemma}
%
Of course, this is too much to ask of a conversion checking algorithm.
% 
Depending on the definition of \( g \), deciding whether \( \mathrm{dec}_g \) is 
convertible to \( 0 \) might amount to deciding whether a Turing machine
halts.
% 
Therefore conversion and typing are undecidable for \SetoidCC + \( \Prop \).

Remark that we can perform the exact same construction in \( \sProp \)
if we add proof-irrelevant natural numbers. 
% 
But it does not cause any decidability issue, as we have a simple way to check 
for convertibility of proof-irrelevant terms: having the same type is 
sufficient! 
% 
For this reason, we never perform reduction in the proof-irrelevant layers, and 
need not worry about these potentially divergent terms.

\paragraph{Definition of an Approximate Fixed Point Combinator}

In an inconsistent context \( \Gamma \), we can use \( \emptyrecName \) to 
derive a term
\[
    e \enskip : \enskip \Depfun[X\ Y]{\Prop}{\Obseq{X}{Y}}
\]
that allows us to freely cast between any two types.
%
We will use it to derive an approximate fixed point for any closed term \( f \) 
of type \( \Fun{(\Fun{\Natprop}{\Natprop})}{(\Fun{\Natprop}{\Natprop})} \).
%
Consider the following definitions: 
\[
  \begin{array}{lcl}
  \Emptyprop &:& \Prop \\
  \Emptyprop &:=& \Depfun[X]{\Prop}{X} \\
  \\
  \Delta_f &:& \Emptyprop \to \Natprop \to \Natprop\\
  \Delta_f &:=& \lam{\bot}{f\ (x\ (\Fun{\Emptyprop}{\Fun{\Natprop}{\Natprop}})\ x)}\\
  \\
  \Delta_f' &:& \Emptyprop \\
  \Delta_f' &:=& \lam[X]{\Prop}{\cast{\Fun{\Emptyprop}{\Fun{\Natprop}{\Natprop}}}{X}{e\ (\Fun{\Emptyprop}{\Fun{\Natprop}{\Natprop}})\ X}{\Delta_f}} \\
  \\
  Y_f &: & \Natprop \to \Natprop \\
  Y_f &:=& \Delta_f\ \Delta_f'
  \end{array}
\]

The term \( Y_f \) is the one that will play the role of a fixed point for \( f \).
% 
It is only an \emph{approximate} fixed point though, because it is not quite 
convertible to \( f\ Y_f \).   
%
Instead, a tedious but straightforward computation using the definitional equality of 
\SetoidCC + \( \Prop \) shows that
% 
\begin{align*}
  \Gamma \vdash Y_f \enskip &\equiv \enskip \Delta_f\ \Delta_f' \quad : \quad \Natprop \to \Natprop\\
      &\Rightarrow^* \enskip f\ (\alpha\ (\Delta_f\ (\beta\ \Delta_f')))\\
      &\Rightarrow^* \enskip f\ (\alpha\ (f\ (\alpha^2\ (\Delta_f\ (\beta^3\ \Delta_f')))))\\
      &\Rightarrow^* \enskip f\ (\alpha\ (f\ (\alpha^2\ (f\ (\alpha^4\ (\Delta_f\ (\beta^7\ \Delta_f')))))))\\
      &\Rightarrow^* \enskip ...
\end{align*}
where we define the auxiliary functions \( \alpha \) and \( \beta \) as:
\[
\begin{array}{lcl}
\alpha & : & (\Fun{\Natprop}{\Natprop}) \to (\Fun{\Natprop}{\Natprop}) \\
\alpha & := & \lambda x.\ {\cast{\Fun{\Natprop}{\Natprop}}{\Fun{\Natprop}{\Natprop}}{e\ ({\Fun{\Natprop}{\Natprop}})\ ({\Fun{\Natprop}{\Natprop}})}{x}} \\
\\
\beta & : & \Emptyprop \to \Emptyprop \\
\beta & := & \lambda x.\ {\cast{\Emptyprop}{\Emptyprop}{e\ \Emptyprop\ \Emptyprop}{x}}.
\end{array}
\]

Thus, \( Y_f \) is only a fixed point combinator up to a bunch of reflexive \( \castName \).
% 
This behavior is sufficient for our purposes, since all the \( \alpha \) disappear
when the function \( Y_f \) is applied to an actual integer.
%
Now, given a closed term \( g \) of type \( \Fun{\Natprop}{\Natprop} \) that
terminates on all canonical inputs, we pose
\[
\begin{array}{lcl}
  f & : & (\Fun{\Natprop}{\Natprop}) \to (\Fun{\Natprop}{\Natprop}) \\
  f & := & \lam[p]{\Fun{\Natprop}{\Natprop}}{\lam[n]{\Natprop}{{\natrec{\Natprop}{0}{p\ (g\ n)}{n}}}}
\end{array}
\]

and \sideremark{Here, the values 0 and 1 should be understood as the corresponding 
  integers in \( \Natprop \).}
% 
we obtain that \( Y_f\ 1 \) reduces to 0 if there exists a \( n \) such that
\( g^n\ 1 \equiv 0 \), but diverges (for any reduction strategy) otherwise.

% By defining $\castName$ using the elimination principle of the
% inductive equality, the same result applies to the
% theory of \sidecitet{lmcs:6606}, even if the reduction of $Y_f$ is
% slightly different.

\paragraph{Conversion is Undecidable}

\SetoidCC is expressive enough to encode Turing machines so that
knowing whether there exists a \( n \) such that \( g^n\ 1 = 0 \) is
undecidable.
%
To complete the proof of undecidability of conversion, we still need
to show that a term of type \( \Natprop \) that diverges for all reduction strategies is not
convertible to $0$. In other words, we need to show that conversion is
not degenerate.

To establish this, we use a lemma of Geuvers, adapted to the 
Calculus of Inductive Constructions by \sidecitet[][lemma 2.19]{wernerthesis}.
The lemma corresponds to a weak form of confluence: any well-typed term
\( t \) that is \( \beta \eta \iota \)-equal to a weak head normal form
\( t' \) actually reduces to an \( \eta \)-expanded form of \( t' \) for 
untyped \( \beta \iota' \) reduction, a slightly modified version of 
untyped \( \beta \iota \) reduction. 
% 
If we take \( t' = 0 \) then we obtain that a well-typed term \( t \) that
is \( \beta \eta \iota \)-equal to 0 has a normal form, and thus cannot be
divergent. 

In order to apply this lemma to \SetoidCC, we need to extend the proof to 
strict propositions and type-casting operators. The extension is not too
difficult: strict propositions do not contribute to the reduction behavior, 
and type-casting is very similar to an eliminator of an inductive type.

\paragraph{Is this situation salvageable?}
% 
As we see, there is no obvious way to define how the observational equality 
should compute between elements of \( \Prop \). 
% 
Is the system \SetoidCC + \( \Prop \) doomed, then?

Long story short, I do not know. Maybe there is a way to define a system that
is better compartmentalized, so that both \( \sProp \) and \( \Prop \) can 
interact with the predicative layer without interacting together...
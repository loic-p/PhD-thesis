\setchapterimage[6cm]{seaside}
\setchapterpreamble[u]{\margintoc}
\chapter{Meta-theory of \SetoidCC}
\labch{metatheory}

In this chapter, we will prove these four properties for \SetoidCC.
%
We first present our extension of the
logical relations framework developed by \sidecitet{Abel:POPL2018} and
later extended to $\sProp$ by~\sidecitet{gilbert:hal-01859964}. The
framework provides a proof of normalization and canonicity from
consistency, as well as decidability of conversion and thus decidability of
type checking.
%
It has been formalized in the \Agda development.
%
Then, to get consistency and therefore canonicity, we develop a model of
\SetoidCC in a constructive set theory that supports inductive-recursive definitions and one
Grothendieck universe.

In this section, we build a model to show normalization and decidability of 
conversion for \SetoidCC, based on the normalization model for \MLTT presented 
by \sidecitet{Abel:POPL2018}.

\section{The Normalization Model}
\label{sec:logic-relat-with}

\subsection{Overview of the Proof}

\paragraph*{Proving Normalization}

In dependent type theory, proving that every well-typed term is normalizing 
is a complex matter.
% 
If we try to prove it by naive induction on the typing derivations, we quickly 
get stuck on the case of the application rule, because we cannot obtain that 
\( t~u \) is normalizing from the hypotheses that $t$ and $u$ are normalizing.
%
Still, reasoning by induction on the typing derivations is pretty much our only 
option, as type theories are fundamentally inductive objects, as presented by 
the inference rules.
% 
Thus what we really need is a better induction hypothesis, one that implies
normalization of well-typed terms but is also preserved by all the inference 
rules of the type theory.

When we are dealing with a type theory that is as complex as \MLTT or \SetoidCC, 
the typical induction hypotheses get very complex, with cases specifically 
tailored to handle each basic type and a lot of interdependence.
% 
At this point, it becomes easier to present the proof as the construction of
a \emph{model} for the type theory: we interpret every type \( A \) as a pair 
of a predicate \( [A] \) that expresses the induction hypothesis for terms of 
type \( A \), and a binary relation \( [A]_= \) that gives the induction 
hypothesis for convertible elements of \( A \).
% 
\sideremark{The two predicates \( [A] \) and \( [A]_= \) could equivalently be 
  presented as a partial equivalence relations on terms, which is the more 
  traditional way to phrase it.}
% 
Next, we show by induction that all typing derivations are valid in the model,
meaning that if \( t : A \) is derivable then \( t \in [A] \) and that if
\( t \equiv u : A \) is derivable then \( [A]_=\ t\ u \).
% 
If we designed our predicates so that they imply normalization, we have our 
proof.
% 
Such models are traditionally built using the \emph{reducibility} technique
of Tait~\sidecite{Tait67}, or its reinterpretation in categorical language
that goes by the name \emph{gluing}~\sidecite{shulman_2015}.

In \sidecite{Abel:POPL2018}, Abel \etal present a normalization model for a 
subset of \MLTT that includes one universe level, dependent products and 
natural numbers with large elimination.
% 
They build their model in intensional type theory with induction-recursion, 
a meta-theory which is remarkably close to the theory being studied.
% 
Furthermore, they entirely formalized their proofs in \Agda, down to the proof
of decidability of the conversion.
% 
In this chapter, we will extend their model to show normalization and decidability
of conversion for the full system \SetoidCC.
% 
We formalized a significant portion of the proof in \Agda, which handles two 
predicative universes, the impredicative universe of propositions, dependent 
products, natural numbers and the observational equality. 
% 
Links to our code will be scattered thoughout the chapter, but 
the main file can be consulted at \refAgdaRoot{Everything}.

\paragraph*{The Model of Abel \etal}
% 
Abel \etal start by defining reducibility, three proof-relevant predicates on untyped 
terms that are associated to ``well-behaved'' types \( A \) of sort \( \Univ \) in 
a context \( \Gamma \):
% 
\begin{itemize}
\item the unary predicate \( {\tytmR{\ell}{\Gamma}{\_}{A}{\Univ}} \) which 
  defines the set of \emph{reducible terms} of type \( A \),
\item the binary relation \( {\eqtmR{\ell}{\Gamma}{\_}{\_}{A}{\Univ}} \), 
  which defines \emph{reducible equality} between two reducible terms of type \( A \),
\item and the unary predicate \( {\eqtyR{\ell}{\Gamma}{A}{\_}{\Univ}} \) which 
  defines the set of types \emph{reducibly equal} to \( A \).
\end{itemize}
% 
Whenever a type \( A \) is equipped with these three reducibility predicates, we write 
\( \tytyR{\ell}{\Gamma}{A}{\Univ} \) and we call \( A \) a \emph{reducible type}.

A reducibility predicate typically characterizes the behaviour that ``well-behaved''
terms of type \( A \) should have: for instance, a term \( t \) is deemed
to be a reducible inhabitant of type \( B \to C \) if it reduces to a term 
\( \bar{t} \) in weak-head normal form, such that \( \bar{t} \) applied to
any reducible term of type \( B \) produces a reducible term of type \( C \)
(up to some technical details).

The reducibility predicates are indexed by a level \( \ell \), which reflects 
the predicative nature of the universe hierarchy of \MLTT: reducibility is 
first defined at level $0$ to characterize types and terms that inhabit the 
smallest universe \( \Type_0 \).
% 
Then Abel \etal use reducibility at level 0 to define reducibility at level 1, 
that applies to types and terms that live in \( \Type_1 \) -- in the case of
the type \( \Type_0 : \Type_1 \), they define
\[
\begin{array}{rcl}
  \tytmR{1}{\Gamma}{A}{\Type_0}{\Type_1} & := & \tytyR{0}{\Gamma}{A}{\Type_0} \\
  \eqtmR{1}{\Gamma}{A}{B}{\Type_0}{\Type_1} & := & \eqtyR{0}{\Gamma}{A}{B}{\Type_0}. 
\end{array}
\]
% 
And this definition is iterated to handle any universe level.

However, reducibility is not quite strong enough to build a model of \MLTT, 
so Abel \etal go on to define \emph{validity} which is the closure 
of reducibility under substitution.
% 
Only after this step can they prove the \emph{fundamental lemma} which shows 
that the validity model supports all the inference rules of their fragment of
\MLTT, and thus that every well-typed term can be interpreted in the validity
model. Since validity implies normalization, this concludes the proof.

\paragraph*{Adding Proof-Irrelevant Types}
% 
In~\sidecite{gilbert:hal-01859964}, Gilbert \etal extend the model of 
Abel \etal to support definitionally proof-irrelevant types.
% 
They achieve this by extending the reduction rules to the inhabitants of 
proof-irrelevant types, and they prove normalization for both relevant
terms and irrelevant proofs.
%
From there, Gilbert \etal derive an easy proof of consistency: any proof of 
the false proposition \( \Empty \) in an an empty context will reduce to a
weak head normal form, and the only weak head normal forms that can inhabit 
\( \Empty \) are neutral terms. 
% 
But since there are no variables in the empty context, neutral terms cannot 
exist, so \( \Empty \) has no inhabitant in the empty context.

However, this strategy is not applicable in our setting. Contrary to the theory
studied by Gilbert \etal, \SetoidCC relies crucially on proof-irrelevant 
axioms, which do not have any clear reduction rule or normal forms.
%
Therefore we drop the idea of having reduction rules for inhabitants of 
propositions, and we only prove normalization for proof-relevant terms.
%
We argue that this is more faithful to the philosophy of computational
irrelevance, and results in a proof that is completely agnostic about
the proof-irrelevant content of the theory -- we could postulate any
consistent proof-irrelevant axiom (even excluded middle!) and the normalization 
proof would carry through just as well.

Unfortunately, this means that consistency and canonicity do not follow
from normalization anymore. The reason is simple: since we gave up any
kind of control on the proof-irrelevant terms, we might have proofs
of \( \Empty \) in the empty context for all we can tell.
% 
But this also weakens our control on neutral terms in the proof-\emph{relevant} 
layer, as \( \Empty \)-elim can build an inhabitant of any proof-relevant type 
from a proof of \( \Empty \).
% 
Therefore, if we want to show canonicity, we need to show that there is no proof 
of \( \Empty \) in the empty context -- in other words, that the theory is 
consistent.

\paragraph*{Adding Support for Impredicativity} 
% 
Impredicativity is famously difficult to model in reducibility proofs, because
it breaks the well-foundedness of the type hierarchy: impredicative dependent
products may have an arbitrarily large codomain, while still being at the 
bottom of the universe hierarchy.
% 
The standard way to build a normalization model that avoids self-referential
problems is to use \emph{reducibility candidates}, the device introduced by 
Girard~\sidecite{girard72} to prove normalization for System F.
% 
Fortunately, in \SetoidCC the impredicative dependent products are 
propositions, and thus proof-irrelevant. This means we do not have to
prove any normalization result for their inhabitants, so we will not need
Girard's reducibility candidates.

However, we still need to define reducibility for inhabitants of \( \sProp \),
which is a proof-relevant type. And since its sort is \( \Type_0 \), the 
reducibility predicates for \( \sProp \) should be defined at level 0, without
any knowledge of the model for higher universes. 
% 
So, when should an impredicative dependent product be reducible?
% 
In the model of Abel \etal, dependent products are deemed reducible when
their domain is reducible, and their codomain is reducible 
for any reducible element of the domain. 
% 
But an impredicative dependent product may have an aribitrarily large domain,
so this definition will not work.
% 
We break out of that circularity by defining a weaker notion of reducibility 
for impredicative types that does not provide any control on the domain and 
codomain of dependent products.

Interestingly, this weakened reducibility means that we cannot prove the 
fundamental lemma directly on the rules we presented in \cref{ch:observational}, 
which omit plenty of well-formedness hypotheses for the sake of readability 
(for instance, the rule~\nameref{infrule:pi-intro} does not ask for 
well-formedness of the involved types).
% 
In their model, Abel \etal were able to collect this redundant info in the 
reducibility predicates, but we are not, so we prove our theorem for more
verbose inference rules.
% 
Once the fundamental lemma is proved, we will be able to show that the rules
of \cref{ch:observational} do in fact contain sufficient information to recover
these additional hypotheses (see \cref{sec:normalization-consequences}).

\subsection{Modelling the Proof-Relevant Layer}

We start by presenting the definition of reducibility for the relevant
fragment, closely following \sidecitet{Abel:POPL2018}.
%
For pedagogical reasons, we will present a somewhat informal version
of the proof where some arguments are left implicit, and encourage the reader 
who is interested in the technical details to have a look at the \Agda 
formalization.
%
While our formal proof and the definition in \refAgda{}{LogicalRelation} only 
model a subset of \SetoidCC with three reducibility levels, it should be quite 
clear that the same technique works for any finite number of levels.

The reducibility predicates are defined by induction-recursion, as detailed
in \cref{fig:logrel-ind-rec}.
%
The reducible types are defined inductively with eleven constructors, one for 
every type former of \SetoidCC and one for neutral types. 
% 
Each constructor takes as argument a proof of an auxiliary predicate of the 
form \( \Gamma \Vdash_X t \) that packs information specific to the 
corresponding type, that we will define individually.
%
The logical relations on terms and equalities are simultaneously
defined by recursion on \( \tytyR{\ell}{\Gamma}{A}{\Univ} \), using
more auxiliary predicates.
%
This technically means that these judgments should have an extra
argument of type \( \tytyR{\ell}{\Gamma}{A}{\Univ} \), but since it
will turn out that all such derivations give rise to equivalent judgments
(cf \refAgda{}{Irrelevance}), we leave this argument implicit.

\begin{figure*}
  \begin{small}
\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ : \_ & : & (\Gamma : \Context) \to (t : \Term) \to (A : \Term) \to (\Univ : \Sort) \to \AgdaSet{} \\
\_ \Vdash_\ell \_ : \_ & \bnfis & \Vne : \forall\ \Gamma\ t\ \Univ \to
                          (\Gamma \Vdash_{\mathsf{ne}} t : \Univ) \to
                          \Gamma \Vdash_\ell t : \Univ\\
               & \sep & \Vforall : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Pi \mathsf{i}} t) \to
                          \Gamma \Vdash_\ell t : \sProp\\
               & \sep & \Vempty : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\bot} t) \to
                          \Gamma \Vdash_\ell t : \sProp\\
               & \sep & \VU : \forall\ \Gamma\ t\ i \to
                          (\Gamma \Vdash_{\mathsf{U}} t : \Type_i) \to
                          \Gamma \Vdash_\ell t : \Type_i\\
               & \sep & \VOmega : \forall\ \Gamma\ t\ i \to
                          (\Gamma \Vdash_{\Omega} t : \Type_i) \to
                          \Gamma \Vdash_\ell t : \Type_i\\
               & \sep & \Vnat : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Nat} t) \to
                          \Gamma \Vdash_\ell t : \Type_0\\
               & \sep & \Vpi : \forall\ \Gamma\ t\ i \to
                          (\Gamma \Vdash_{\Pi} t : \Type_i) \to
                          \Gamma \Vdash_\ell t : \Type_i\\
               & \sep & \Vsigma : \forall\ \Gamma\ t\ i \to
                          (\Gamma \Vdash_{\Sigma} t : \Type_i) \to
                          \Gamma \Vdash_\ell t : \Type_i\\
               & \sep & \Vbox : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Box} t) \to
                          \Gamma \Vdash_\ell t : \Type_0\\
               & \sep & \Vquo : \forall\ \Gamma\ t\ i \to
                          (\Gamma \Vdash_{\mathsf{Q}} t : \Type_i) \to
                          \Gamma \Vdash_\ell t : \Type_i\\
               & \sep & \Vid : \forall\ \Gamma\ t\ i \to
                          (\Gamma \Vdash_{\mathsf{Id}} t : \Type_i) \to
                          \Gamma \Vdash_\ell t : \Type_i\\
               %% & \sep & \Vemb : \forall\ \ell'\ \Gamma\ t\ s \to
               %%            \ell' < \ell \to
               %%            \Gamma \Vdash_{\ell'} t : s \to
               %%            \Gamma \Vdash_{\ell} t : s\\
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ \equiv \_ : \_ & : & (\Gamma : \Context) \to (A : \Term) \to (B : \Term) \to (s : \Sort) \to \{ \Gamma \Vdash_\ell A : s \} \to \AgdaSet{} \\
\Gamma \Vdash_\ell A \equiv B : s\ \{ \VX \} & = & \Gamma \Vdash_{X} A \equiv B : s
  \qquad \text{for all }\ X \in \{\mathsf{ne}, {\Pi \mathsf{i}}, \bot, \mathsf{U}, \Omega, \mathbb{N}, \Pi, \Sigma, \Box, \mathsf{Q}, \mathsf{Id} \}
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ : \_ : \_ & : & (\Gamma : \Context) \to (t : \Term) \to (A : \Term) \to (s : \Sort) \to \{ \Gamma \Vdash_\ell A : s \} \to \AgdaSet{} \\
\Gamma \Vdash_\ell t : A : s\ \{ \VX \} & = & \Gamma \Vdash_{X} t : A: s
  \qquad \text{for all }\ X \in \{\mathsf{ne}, {\Pi \mathsf{i}}, \bot, \mathsf{U}, \Omega, \mathbb{N}, \Pi, \Sigma, \Box, \mathsf{Q}, \mathsf{Id} \}
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ \equiv \_ : \_ : \_ & : & (\Gamma : \Context) \to (t\ u : \Term) \to (A : \Term) \to (s : \Sort) \to \{ \Gamma \Vdash_\ell A : s \} \to \AgdaSet{} \\
\Gamma \Vdash_\ell t \equiv u : A : s\ \{ \VX \} & = & \Gamma \Vdash_{X} t \equiv u : A: s
  \qquad \text{for all }\ X \in \{\mathsf{ne}, {\Pi \mathsf{i}}, \bot, \mathsf{U}, \Omega, \mathbb{N}, \Pi, \Sigma, \Box, \mathsf{Q}, \mathsf{Id} \}
  \end{array} &&
\end{flalign*}
\end{small}
  \caption{Inductive-recursive presentation of reducibility}
  \label{fig:logrel-ind-rec}
\end{figure*}

%% As is customary in reducibility proofs~\sidecite{girard72}, these
%% judgments imply normalization, are closed under weak head expansion,
%% and are verified by all neutral terms.

We now give the definition of all these auxiliary predicates, starting
with the predicates for neutral types.

\paragraph{Proof-relevant Neutral Types}

% The first kind of reducible type are neutral type.
\[
  \inferrule{\redT{\Gamma}{A}{N}{\Type_i} \\ \neutral{N}}
  {\tytyR{\mathsf{ne}}{\Gamma}{A}{\Type_i}}
\]
Terms which reduce to neutral types are reducible types.
In this rule, the premise \( \redT{\Gamma}{A}{N}{\Type_i} \) means that \( A \) 
reduces to a term \( N \) in a finite number of steps (possibly zero), and that 
both \( A \) and \( N \) are of sort \( \Type_i \) in context \( \Gamma \).
%
When $A$ is reducible to a neutral type, we also define:
% Given a derivation
% \( \metatm{\mathscr{A}}{\tytyR{\ell}{\Gamma}{A}} \) built by this rule, we define:
\begin{itemize}
  \item \( \eqtyR{\mathsf{ne}}{\Gamma}{A}{B}{\Type_i} \) if there is a neutral term \( M \) such that
    \( \redT{\Gamma}{B}{M}{\Type_i} \) and \( \eqtm{\Gamma}{N}{M}{\Type_i} \).
  \item \( \tytmR{\mathsf{ne}}{\Gamma}{t}{A}{\Type_i} \) if there is a neutral term \( n \) such that
    \( \redT{\Gamma}{t}{n}{N} \).
  \item \( \eqtmR{\mathsf{ne}}{\Gamma}{t}{u}{A}{\Type_i} \) if there are neutral terms \( n, m \) such that
    \( \redT{\Gamma}{t}{n}{N} \) and \( \redT{\Gamma}{u}{m}{N} \), and
    \( \eqtm{\Gamma}{n}{m}{N} \).
\end{itemize}

In other words, being a reducible inhabitant of a neutral type is simply
reducing to a neutral term.

\paragraph{Natural Numbers}
\[
  \inferrule{\redT{\Gamma}{A}{\Nat}{\Type_0}}
            {\Gamma \Vdash_{\mathbb{N}} A}
\]
Terms which reduce to $\Nat$ are reducible types. In case \( A \) is reducible 
to \( \Nat \), we also define:
 % Given a derivation \( \metatm{\mathscr{A}}{\tytyR{\ell}{\Gamma}{A}} \) built by this rule, we
% define:
\begin{itemize}
  \item \( \Gamma \Vdash_{\mathbb{N}} A \equiv B \) if \( \redT{\Gamma}{B}{\Nat}{\Type_0} \).
  \item \( \Gamma \Vdash_{\mathbb{N}} t : A \) if there is a normal form \( t' \) such that
    \( \redT{\Gamma}{t}{t'}{\Nat} \) and \( \Gamma \Vdash_{\mathbb{N}\mathsf{t}} t' \), which is inductively
    defined by
    \[
      \inferrule{ }
                {\nattmR{\Gamma}{\zero}}
      \quad
      \inferrule{\nattmR{\Gamma}{t}}
                {\nattmR{\Gamma}{\suc{t}}}
      \quad
      \inferrule{\tytm{\Gamma}{n}{\Nat}
                \\ \text{\( n \) is neutral}}
                {\nattmR{\Gamma}{n}}
    \]
  \item \( \Gamma \Vdash_{\mathbb{N}} t \equiv u : A \) if there are normal forms \( t', u' \) such that
    \( \redT{\Gamma}{t}{t'}{\Nat} \) and
    \( \redT{\Gamma}{u}{u'}{\Nat} \), and
    \( \nateqR{\Gamma}{t'}{u'} \), which is inductively defined by
\begin{mathpar}
      \inferrule{ }
                {\nateqR{\Gamma}{\zero}{\zero}}
      \and
      \inferrule{\nateqR{\Gamma}{t}{u}}
                {\nateqR{\Gamma}{\suc{t}}{\suc{u}}}
      \and
      \inferrule{\eqtm{\Gamma}{n}{m}{\Nat}
                \\ \text{\( n, m \) are neutral}}
                {\nateqR{\Gamma}{n}{m}}
\end{mathpar}
\end{itemize}

These definitions mean that a term is a reducible natural number when it 
reduces to either zero, a neutral term or a successor of a reducible 
natural number.

\paragraph{Predicative Universes}
\[
  \inferrule{\redT{\Gamma}{A}{\Type_i{}}{\Type_{i+1}}}
            {\tytyR{\mathsf{U}}{\Gamma}{A}{\Type_{i+1}}}
            { \substack{i < \ell}}
\]
Terms which reduce to a predicative universe are reducible types. In case \( A \) 
is reducible to a predicative universe, we also define:
\begin{itemize}
  \item \( \eqtyR{\mathsf{U}}{\Gamma}{A}{B}{\Type_{i+1}} \) if \( \redT{\Gamma}{B}{\Type_i{}}{\Type_{i+1}} \).
  \item \( \tytmR{\mathsf{U}}{\Gamma}{t}{A}{\Type_{i+1}} \) if there is a normal form \( t' \) such that
    \( \redT{\Gamma}{t}{t'}{\Type_i{}} \), and \( \tytyR{i}{\Gamma}{t}{\Type_i} \)

    (which is already defined by induction hypothesis, since
    \( i < \ell \)).
  \item \( \eqtmR{\mathsf{U}}{\Gamma}{t}{u}{A}{\Type_{i+1}} \) if there are normal forms \( t', u' \) such that
    \( \redT{\Gamma}{t}{t'}{\Type_i{}} \)
    and \( \redT{\Gamma}{u}{u'}{\Type_i{}} \), and
    \( \tytyR{i}{\Gamma}{t}{\Type_i} \), \( \tytyR{i}{\Gamma}{u}{\Type_i} \), and \( \eqtyR{i}{\Gamma}{t}{u}{\Type_i} \).
\end{itemize}

According to these definitions, being a reducible inhabitant of a predicative
universe is the same as being a proof-relevant reducible type: either a 
neutral, or a dependent product, etc.
% 
This is the only case that recursively calls the definition of reducibility
at a lower level, forcing us to define the whole affair by induction on 
\( \ell \).

\paragraph{Proof-relevant Dependent Products}

  \begin{mathpar}
  \inferrule{\redT{\Gamma}{A}{\Depfunannotated{F}{G}{}{\Univ}{\Type_i}}{\Type_{\mathrm{max}(\Univ, i)}}
            \\ \tytm{\Gamma}{F}{\Univ}
            \\ \tytm{\extctx{\Gamma}{F}}{G}{\Type_i}
            \\ \forall \wkrho,\ \tytyR{\ell}{\Delta}{F[\rho]}{\Univ}
            \\ \forall \wkrho,\ \metaimply{\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Univ}}{\tytyR{\ell}{\Delta}{G[\rho, a]}{\Type_i}}
            \\ \forall \wkrho,
              {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Univ}}
              \to {\tytmR{\ell}{\Delta}{b}{F[\rho]}{\Univ}}
              \to {\eqtmR{\ell}{\Delta}{a}{b}{F[\rho]}{\Univ}}
              \to {\eqtyR{\ell}{\Delta}{G[\rho, a]}{G[\rho, b]}{\Type_i}}}
            {\tytyR{\Pi}{\Gamma}{A}{\Type_{\mathrm{max}(\Univ, i)}}}
  \end{mathpar}
%
This rule describes the condition for $A$ to be reducible to a proof-relevant
dependent product.
%
We introduced quite a bit of notation here, so we go over the premises one by 
one.
% 
The first three premises state that \( A \) reduces to a dependent product in a
finite number of steps, and that the domain and codomain of the dependent 
product are well-formed types.
% 
\sideremark{Despite the similarity with an inclusion symbol, the notation 
\( \Delta âŠ‘ \Gamma \) means that all types of \( \Gamma \) appear in 
\( \Delta \).}
% 
The fourth premise asks for the domain \( F \) to be a reducible type under any
weakening: the notation \( \wkrho \) means that \( \rho \) is a weakening from
a context \( \Delta \) to the context \( \Gamma \), and the notation 
\( F[\rho] \) represents the result of weakening the free variables of \( F \).
% 
The fifth premise says that given any weakening \( \rho \) and a term \( a \)
which is a reducible inhabitant of \( F[\rho] \), the term we obtain by
applying the weakening \( \rho \) to all free variables of \( G \) except for
\( x \), and then substituting \( a \) for \( x \) is reducible.
This term is noted \( G[\rho, a] \). 
% 
Finally, the last premise states that under any weakening, applying \( G \) to
reducibly equal inhabitants of \( F \) results in two reducibly equal types.

\sideremark[*1]{Thus, reducibility has the structure of a \emph{presheaf} over the
category of weakenings. These generalized hypotheses are really an embodiment of
the presheaf exponential.}
The reader might be wondering why do we generalize the last three premises 
under any weakening. The simple explanation is that we want reducibility to be
stable under weakening, and we won't be able to prove it by induction on the
definition because of the negative occurences -- so we generalize any premise
that mentions reducibility on the left of an arrow.

When $A$ is reducible to a dependent product, we define:
\begin{itemize}
  \item \( \eqtyR{\Pi}{\Gamma}{A}{B}{\Type_{\mathrm{max}(\Univ, i)}} \) if there are terms \( F' \) and \( G' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{B}{\Depfun{F'}{G'}}{\Type_{\mathrm{max}(\Univ, i)}} \) 
      \item \( \eqtm{\Gamma}{\Depfun{F}{G}}{\Depfun{F'}{G'}}{\Type_{\mathrm{max}(\Univ, i)}} \)
      \item \( \forall \wkrho.\ \ \eqtyR{\ell}{\Delta}{F[\rho]}{F'[\rho]}{\Univ} \)
      \item \( \forall \wkrho.\ \ {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Univ}} \) 
            \\ \( \phantom{\forall \wkrho} \to {\eqtyR{\ell}{\Delta}{G[\rho, a]}{G'[\rho, a]}{\Type_i}} \).
      \end{itemize}
    \item \( \tytmR{\Pi}{\Gamma}{t}{A}{{\Type_{\mathrm{max}(\Univ, i)}}} \) if there is a normal form \( t' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{t}{t'}{\Depfun{F}{G}} \)
      \item \( \forall \wkrho.\ \ {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Univ}} \) 
            \\ \( \phantom{\forall \wkrho} \to {\tytmR{\ell}{\Delta}{t'[\rho]\ a}{G[\rho, a]}{\Type_i}} \)
      \item \( \forall \wkrho.\ {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Univ}} \)
            \\ \( \phantom{\forall \wkrho} \to {\tytmR{\ell}{\Delta}{b}{F[\rho]}{\Univ}} \to {\eqtmR{\ell}{\Delta}{a}{b}{F[\rho]}{\Univ}} \)
            \\ \( \phantom{\forall \wkrho} \to {\eqtmR{\ell}{\Delta}{t'[\rho]\ a}{t'[\rho]\ b}{G[\rho,a]}{\Type_i}} \).
    \end{itemize}
  \item \( \eqtmR{\Pi}{\Gamma}{t}{u}{A}{\Type_{\mathrm{max}(\Univ, i)}} \) if there are normal forms \( t', u' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{t}{t'}{\Depfun{F}{G}} \)
      and \( \redT{\Gamma}{u}{u'}{\Depfun{F}{G}} \)
      \item \( \eqtm{\Gamma}{t'}{u'}{\Depfun{F}{G}} \)
      % \item \( \tytmR{\Pi}{\Gamma}{t}{A}{\Type_{\mathrm{max}(\Univ, i)}} \) 
      %       and \( \tytmR{\Pi}{\Gamma}{u}{A}{\Type_{\mathrm{max}(\Univ, i)}} \)
      \item \( \forall \wkrho.\ \ {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Univ}} \)
            \\ \( \phantom{\forall \wkrho} \to {\eqtmR{\ell}{\Delta}{t'[\rho]\ a}{u'[\rho]\ a}{G[\rho, a]}{\Type_i}} \).
    \end{itemize}
  \end{itemize}

These definitions deem a term to be a reducible dependent function when it
reduces to either a neutral term or a lambda-abstraction, and it sends 
reducible inhabitant of the domain to reducible inhabitants of the codomain.
% 
Note that in the three definitions above, we talk about reducibility of 
inhabitants of the domain and the codomain. 
We know that these reducibility predicates are well-defined because we assumed
that the domain and the codomain are reducible types when we supposed
that \( A \) is reducible to a dependent product.

\paragraph{Dependent Sums}

\begin{mathpar}
\inferrule{\redT{\Gamma}{A}{\Depsumannotated{F}{G}{}{\Type_i}{\Type_j}}{\Type_{\mathrm{max}(i,j)}}
          \\ \tytm{\Gamma}{F}{\Type_i}
          \\ \tytm{\extctx{\Gamma}{F}}{G}{\Type_j}
          \\ \forall \wkrho,\ \tytyR{\ell}{\Delta}{F[\rho]}{\Type_i}
          \\ \forall \wkrho,\ \metaimply{\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Type_i}}{\tytyR{\ell}{\Delta}{G[\rho, a]}{\Type_j}}
          \\ \forall \wkrho,
            {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Type_i}}
            \to {\tytmR{\ell}{\Delta}{b}{F[\rho]}{\Type_i}}
            \to {\eqtmR{\ell}{\Delta}{a}{b}{F[\rho]}{\Type_i}}
            \to {\eqtyR{\ell}{\Delta}{G[\rho, a]}{G[\rho, b]}{\Type_j}}}
          {\tytyR{\Sigma}{\Gamma}{A}{\Type_{\mathrm{max}(i,j)}}}
\end{mathpar}

When \( A \) reduces to a dependent sum \( \Depsum{F}{G} \) such that \( F \) 
and \( G \) verify the same conditions as for the dependent product, then 
\( A \) is a reducible type.
% 
In this case, we define:
\begin{itemize}
  \item \( \eqtyR{\Sigma}{\Gamma}{A}{B}{\Type_{\mathrm{max}(i,j)}} \) if there are terms \( F' \) and \( G' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{B}{\Depsum{F'}{G'}}{\Type_{\mathrm{max}(i,j)}} \) 
      \item \( \eqtm{\Gamma}{\Depsum{F}{G}}{\Depsum{F'}{G'}}{\Type_{\mathrm{max}(i,j)}} \)
      \item \( \forall \wkrho.\ \ \eqtyR{\ell}{\Delta}{F[\rho]}{F'[\rho]}{\Type_i} \)
      \item \( \forall \wkrho.\ \ {\tytmR{\ell}{\Delta}{a}{F[\rho]}{\Type_i}} \) 
            \\ \( \phantom{\forall \wkrho} \to {\eqtyR{\ell}{\Delta}{G[\rho, a]}{G'[\rho, a]}{\Type_j}} \).
      \end{itemize}
    \item \( \tytmR{\Sigma}{\Gamma}{t}{A}{{\Type_{\mathrm{max}(i,j)}}} \) if there is a normal form \( t' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{t}{t'}{\Depsum{F}{G}} \)
      \item \( \forall \wkrho.\ \ {\tytmR{\ell}{\Delta}{\relfst{t'[\rho]}}{F[\rho]}{\Type_i}} \) 
      \item \( \forall \wkrho.\ \ {\tytmR{\ell}{\Delta}{\relsnd{t'[\rho]}}{G[\rho, \relfst{t'[\rho]}]}{\Type_j}}. \)
    \end{itemize}
  \item \( \eqtmR{\Sigma}{\Gamma}{t}{u}{A}{\Type_{\mathrm{max}(i,j)}} \) if there are normal forms \( t', u' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{t}{t'}{\Depsum{F}{G}} \)
      and \( \redT{\Gamma}{u}{u'}{\Depsum{F}{G}} \)
      \item \( \eqtm{\Gamma}{t'}{u'}{\Depsum{F}{G}} \)
      % \item \( \tytmR{\Sigma}{\Gamma}{t}{A}{\Type_{\mathrm{max}(i,j)}} \) 
      %       and \( \tytmR{\Sigma}{\Gamma}{u}{A}{\Type_{\mathrm{max}(i,j)}} \)
      \item \( \forall \wkrho.\ \ {\eqtmR{\ell}{\Delta}{\relfst{t'[\rho]}}{\relfst{u'[\rho]}}{F[\rho]}{\Type_i}} \) 
      \item \( \forall \wkrho. \) \\ \({}\quad {\eqtmR{\ell}{\Delta}{\relsnd{t'[\rho]}}{\relsnd{u'[\rho]}}{G[\rho, \relfst{t'[\rho]}]}{\Type_j}}. \)
    \end{itemize}
\end{itemize}
Thus, a term is a reducible inhabitant of \( A \) when it reduces to either a 
neutral term or a dependent pair, a both of its projections are reducible.

\paragraph*{Box types}
\[
  \inferrule{\redT{\Gamma}{A}{\Boxt{A'}}{\Type_0}
            \\ \tytm{\Gamma}{A'}{\sProp}
            \\ \tytyR{\ell}{\Gamma}{A'}{\sProp}}
            {\tytyR{\Box}{\Gamma}{A}{\Type_0}}
\]
If \( A \) reduces to a boxed reducible proposition, then \( A \) is a 
reducible type. In which case, we define:
\begin{itemize}
  \item \( \eqtyR{\Box}{\Gamma}{A}{B}{\Type_0} \) if there is a term \( B' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{B}{\Boxt{B'}}{\Type_0} \)
      \item \( \eqtyR{\ell}{\Gamma}{A'}{B'}{\sProp} \)
    \end{itemize}
  \item \( \tytmR{\Box}{\Gamma}{t}{A}{\Type_0} \) if there is a normal form \( t' \) such that
    \( \redT{\Gamma}{t}{t'}{\Boxt{A'}} \) and \( \boxtmR{\Gamma}{t'} \), which is defined by
    \begin{mathpar}
      \inferrule{\tytm{\Gamma}{t}{A'}}
                {\boxtmR{\Gamma}{\boxt{t}}}
      \and
      \inferrule{\tytm{\Gamma}{n}{\Boxt{A'}}
                \\ \text{\( n \) is neutral}}
                {\boxtmR{\Gamma}{n}}
    \end{mathpar}
  \item \( \eqtmR{\Box}{\Gamma}{t}{u}{A}{\Type_i} \) if there are normal forms \( t', u' \) such that
    \( {\redT{\Gamma}{t}{t'}{\Boxt{A'}}} \), \( {\redT{\Gamma}{u}{u'}{\Boxt{A'}}} \), and
    \( {\boxeqR{\Gamma}{t'}{u'}} \), which is defined by
    \begin{mathpar}
      \inferrule{\tytm{\Gamma}{t}{A'} \\ \tytm{\Gamma}{u}{A'}}
                {\boxeqR{\Gamma}{\boxt{t}}{\boxt{u}}}
      \and
      \inferrule{\eqtm{\Gamma}{n}{m}{\Boxt{A'}}
                \\ \text{\( n, m \) are neutral}}
                {\boxeqR{\Gamma}{n}{m}}
    \end{mathpar}
\end{itemize}
In other words, an inhabitant of a box type is reducible when it reduces
to either a boxed proof, or a neutral term.

\paragraph*{Quotients}

\[
  \inferrule{\redT{\Gamma}{A}{\Quo{A'}{(R,R_r,R_s,R_t)}}{\Type_i}
            \\ \tytm{\Gamma}{A'}{\Type_i}
            \\ \forall \wkrho.\ \ \tytyR{\ell}{\Gamma}{A'[\rho]}{\Type_i}
            \\ \tytmR{\ell}{\Gamma}{R}{\Fun{A'}{\Fun{A'}{\sProp}}}{\Type_i}
            \\ \tytm{\Gamma}{R_r}{\Depfun{A'}{R\ x\ x}}
            \\ \tytm{\Gamma}{R_s}{\Depfun[x,y]{A'}{\Fun{R\ x\ y}{R\ y\ x}}}
            \\ \tytm{\Gamma}{R_t}{\Depfun[x,y,z]{A'}{\Fun{R\ x\ y}{\Fun{R\ y\ z}{R\ x\ z}}}}}
            {\tytyR{\mathsf{Q}}{\Gamma}{A}{\Type_i}}
\]
This rule stipulates that \( A \) is a reducible type if it reduces to a 
quotient type \( \Quo{A'}{R} \), its underlying type \( A' \) is reducible 
under any substitution, and \( R \) is a reducible relation on \( A' \). 
Note that to enunciate this last condition, we need to make sure that the
type of relations on \( A' \) is reducible, which is deduced from the
reducibility of \( A' \) and of \( \sProp \).

If \( A \) is reducible to a quotient type, we define:
\begin{itemize}
  \item \( \eqtyR{\mathsf{Q}}{\Gamma}{A}{B}{\Type_i} \) if there are terms \( B', Q, Q_r, Q_s, Q_t \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{B}{\Quo{B'}{(Q,Q_r,Q_s,Q_t)}}{\Type_i} \)
      \item \( \forall \wkrho.\ \ \eqtyR{\ell}{\Delta}{A'[\rho]}{B'[\rho]}{\Type_i} \)
      \item \( \eqtmR{\ell}{\Gamma}{R}{Q}{\Fun{A'}{\Fun{A'}{\sProp}}}{\Type_i} \).
      % \item \( \tytm{\Gamma}{Q_r}{\Depfun{B'}{Q\ x\ x}} \)
      % \item \( \tytm{\Gamma}{Q_s}{\Depfun[x,y]{B'}{\Fun{Q\ x\ y}{Q\ y\ x}}} \)
      % \item \( \tytm{\Gamma}{Q_t}{\Depfun[x,y,z]{B'}{\Fun{Q\ x\ y}{\Fun{Q\ y\ z}{Q\ x\ z}}}}} \)
    \end{itemize}
  \item \( \tytmR{\mathsf{Q}}{\Gamma}{t}{A}{\Type_i} \) if there is a normal form \( t' \) such that
    \( \redT{\Gamma}{t}{t'}{\Quo{A'}{R}} \) and \( \quotmR{\Gamma}{t'} \), which is defined by
    \begin{mathpar}
      \inferrule{\tytmR{\ell}{\Gamma}{t}{A'}{\Type_i}}
                {\quotmR{\Gamma}{\quo{t}}}
      \and
      \inferrule{\tytm{\Gamma}{n}{\Quo{A'}{R}}
                \\ \text{\( n \) is neutral}}
                {\quotmR{\Gamma}{n}}
    \end{mathpar}
  \item \( \eqtmR{\mathsf{Q}}{\Gamma}{t}{u}{A}{\Type_0} \) if there are normal forms \( t', u' \) such that
    \( {\redT{\Gamma}{t}{t'}{\Quo{A'}{R}}} \), \( {\redT{\Gamma}{u}{u'}{\Quo{A'}{R}}} \), and
    \( {\quoeqR{\Gamma}{t'}{u'}} \), which is defined by
    \begin{mathpar}
      \inferrule{\eqtmR{\ell}{\Gamma}{t}{u}{A'}{\Type_i}}
                {\quoeqR{\Gamma}{\quo{t}}{\quo{u}}}
      \and
      \inferrule{\eqtm{\Gamma}{n}{m}{\Quo{A'}{R}}
                \\ \text{\( n, m \) are neutral}}
                {\quoeqR{\Gamma}{n}{m}}
    \end{mathpar}
\end{itemize}
In other words, an inhabitant of a quotient type is reducible when it reduces
to either a projection of a reducible term, or a neutral term.

\paragraph*{Inductive Identity Types}

\[
  \inferrule{\redT{\Gamma}{A}{\Id{A'}{t}{u}}{\Type_i}
            \\ \tytyR{\ell}{\Gamma}{A'}{\Type_i}
            \\ \tytmR{\ell}{\Gamma}{t}{A'}{\Type_i}
            \\ \tytmR{\ell}{\Gamma}{u}{A'}{\Type_i}}
            {\tytyR{\mathsf{Id}}{\Gamma}{A}{\Type_i}}
\]
When $A$ reduces to an identity type between two reducible inhabitants of
a reducible type, then \( A \) is reducible. In this case, we define:
\begin{itemize}
  \item \( \eqtyR{\mathsf{Id}}{\Gamma}{A}{B}{\Type_i} \) if there are terms \( B', t', u' \) such that
    \begin{itemize}
      \item \( \redT{\Gamma}{B}{\Id{B'}{t'}{u'}}{\Type_i} \)
      \item \( \eqtyR{\ell}{\Gamma}{A'}{B'}{\Type_i} \)
      \item \( \eqtmR{\ell}{\Gamma}{t}{t'}{A'}{\Type_i} \)
      \item \( \eqtmR{\ell}{\Gamma}{u}{u'}{A'}{\Type_i} \).
    \end{itemize}
  \item \( \tytmR{\mathsf{Id}}{\Gamma}{e}{A}{\Type_i} \) if there is a normal form \( e' \) such that
    \( \redT{\Gamma}{e}{e'}{\Id{A'}{t}{u}} \) and \( \idtmR{\Gamma}{e'} \), which is defined by
    \begin{mathpar}
      \inferrule{ }
                {\idtmR{\Gamma}{\id{t}}}
      \and
      \inferrule{\tytm{\Gamma}{e}{\Obseq[A']{t}{u}}}
                {\idtmR{\Gamma}{\idpath{e}}}
      \and
      \inferrule{\tytm{\Gamma}{n}{\Id{A'}{t}{u}}
                \\ \text{\( n \) is neutral}}
                {\idtmR{\Gamma}{n}}
    \end{mathpar}
  \item \( \eqtmR{\mathsf{Id}}{\Gamma}{e}{f}{A}{\Type_i} \) if there are normal forms \( e', f' \) such that
    \( \redT{\Gamma}{e}{e'}{\Id{A'}{t}{u}} \) and \( \redT{\Gamma}{f}{f'}{\Id{A'}{t}{u}} \), and
    \( \ideqR{\Gamma}{e'}{f'} \), which is inductively defined by
    \begin{mathpar}
      \inferrule{ }
                {\ideqR{\Gamma}{\id{t}}{\id{t}}}
      \and
      \inferrule{\tytm{\Gamma}{e, f}{\Obseq[A']{t}{u}}}
                {\ideqR{\Gamma}{\idpath{e}}{\idpath{f}}}
    \end{mathpar}
    \begin{mathpar}
      \inferrule{\eqtm{\Gamma}{n}{m}{\Id{A'}{t}{u}}
                \\ \text{\( n, m \) are neutral}}
                {\ideqR{\Gamma}{n}{m}}
    \end{mathpar}
  \end{itemize}

Which states that a term is a reducible witness of the inductive equality when it
reduces to either reflexivity, \( \metaop{Idpath} \), or a neutral term.

\subsection{Modelling the Proof-Irrelevant Layer}

We now turn to the definition of reducibility for the proof-irrelevant
fragment, which must be defined independently from the rest of the model
to ensure a well-founded definition.

Since there is no computation whatsoever in proof-irrelevant types,
their inhabitants don't interact with other terms. This allows us
to give a generic definition for reducibility of terms and term
equality, that will work for any \( X \in \{ \bot, {\Pi \mathsf{i}}\} \) :
%
\begin{itemize}
  \item \( \tytmR{X}{\Gamma}{t}{A}{\sProp}\ \) when \(\ \tytm{\Gamma}{t}{A} \).
  \item \( \eqtmR{X}{\Gamma}{t}{u}{A}{\sProp}\ \) when \(\ \tytm{\Gamma}{t}{A} \) and \( \tytm{\Gamma}{u}{A} \).
  \end{itemize}
%
which means that if \( A \) is in \( \sProp \), then the model does not
need to collect any information on inhabitants of
\( A \), save for the fact that they are well-typed. And this applies
to reducible equality too, for any two inhabitants of \( A \) are always
convertible.
% 
It only remains to define the reducibility of types and type equality
for $\Empty$ and the impredicative dependent products.

Now, the reader probably noticed that not all types appear in our definition
of reducibility.
% 
First, note that we do not need to account for \( \Unit \), existential types 
or propositional truncation, as they are defined in terms of \( \Empty \)
and dependent products.
%
Second, the observational equality does not appear in the definition
either.
This is because it does not play the role of a type constructor, but
rather that of a destructor that computes by pattern-matching on types.
Therefore, a type of the shape \( \Obseq[A]{t}{u} \) is never in normal
form, unless if it is neutral (in which case the generic rule for neutral
terms applies).

\paragraph{Empty type}

The case of the empty type is easy, as it does not recursively call
the logical relation.
\[
{\small
  \inferrule{\redT{\Gamma}{A}{\Empty}{\sProp}}
            {\Gamma \Vdash_{\bot} A}
          }\]
%
        When $A$ is reducible to the empty type, we define
        \( \Gamma \Vdash_{\bot} A \equiv B \) as \( \redT{\Gamma}{B}{\Empty}{\sProp} \).


\paragraph{Impredicative Dependent Function Types}

For the impredicative function types, the situation is more complex,
as we can not reproduce the definition of their predicative
counterpart:
it involves a recursive call to the logical relation for the domain
and codomain types, which might live in a higher universe than the
function type.
%
Consequently, we go for minimalism and only collect the fact that the domain
and the codomain are well-typed.

  \[
{\small
  \inferrule{\redT{\Gamma}{A}{\Depfunannotated{F}{G}{}{\Univ}{\sProp}}{\sProp}
            \\ \tytm{\Gamma}{F}{\Type{}}
            \\ \tytm{\extctx{\Gamma}{F}}{G}{\sProp}}
            {\Gamma \Vdash_{\Pi \mathsf{i}} A}
          }\]
%
Similarly, when $A$ is reducible to an impredicative function type,
% Given a derivation \( \metatm{\mathscr{A}}{\tytyR{\ell}{\Gamma}{A}}
% \) built by this rule,
we define reducible equality of $A$ and $B$ as the fact that $B$
reduces to a convertible impredicative function type:
  \[
{\small
  \inferrule{ \redT{\Gamma}{B}{\Depfun{F'}{G'}}{\sProp}
            \\  \eqtm{\Gamma}{\Depfun{F}{G}}{\Depfun{F'}{G'}}{\sProp}}
            {\Gamma \Vdash_{\Pi \mathsf{i}} A \equiv B}
          }\]
        %

These definitions do not recursively call the logical relation,
but as a result the collected invariants are
much weaker than those for relevant dependent function
types. We will discuss this in the proof of the fundamental lemma in \cref{sec:fundamental-lemma}.

\paragraph{The Impredicative Universe}
\[
  \inferrule{\redT{\Gamma}{A}{\sProp}{\Type_0}}
            {\tytyR{\Omega}{\Gamma}{A}{\Type_0}}
\]
When $A$ is reducible to the impredicative universe, we define:
\begin{itemize}
  \item \( \eqtyR{\Omega}{\Gamma}{A}{B}{\Type_i} \) if \( \redT{\Gamma}{B}{\sProp}{\Type_i} \).
  \item \( \tytmR{\Omega}{\Gamma}{t}{A}{\Type_j} \) if there is a normal form \( t' \) such that
    \( \redT{\Gamma}{t}{t'}{\sProp} \), and \( \tytyR{i}{\Gamma}{t}{\sProp} \)

    (which is already defined as we defined the reducibility of impredicative types  
    outside of the large inductive-recursive definition of reducibility).
  \item \( \eqtmR{\Omega}{\Gamma}{t}{u}{A}{\Type_j} \) if there are normal forms \( t', u' \) such that
    \begin{itemize}
    \item \( \redT{\Gamma}{t}{t'}{\sProp} \) and \( \redT{\Gamma}{u}{u'}{\sProp} \)
    \item \( \tytyR{i}{\Gamma}{t}{\sProp} \) and \( \tytyR{i}{\Gamma}{u}{\sProp} \)
    \item  \( \eqtyR{i}{\Gamma}{t}{u}{\sProp} \).
    \end{itemize}
\end{itemize}

\subsection{The Fundamental Lemma}
\label{sec:fundamental-lemma}

The fundamental lemma expresses the completeness of the logical
relation with respect to
typing~\refAgda{LogicalRelation.}{Fundamental}.
%
But before starting our induction, we need to generalize our
hypothesis---the logical relation---yet a bit more: we will prove
reducibility under any well-formed substitution.
%
Following \sidecite{Abel:POPL2018}, we use induction-recursion to define
the notion of \emph{validity}, which will serve as our actual induction
hypothesis \refAgda{LogicalRelation.}{Substitution}.
%

\[ {\small \inferrule{\forall \Delta\ \sigma \to \
      \metaimply{\ctxV{\Delta}}{\metaimply{\tytyS{\Delta}{\sigma}{\Gamma}}{
          \left \{ \begin{array}{l}
                     \tytyR{\ell}{\Delta}{A[\sigma]}{\Type{}} \\
                     \forall \sigma' \to
                     \metaimply{\tytyS{\Delta}{\sigma'}{\Gamma}
                     }{\metaimply{\eqtyS{\Delta}{\sigma}{\sigma'}{\Gamma}}{\eqtyR{\ell}{\Delta}{A[\sigma]}{A[\sigma']}{\Type{}}
                     }}
               \end{array}\right . }}
}
{\tytyV{\ell}{\Gamma}{A}{\Type{}}}
}\]
%
In the definition above, $\Delta$ is valid context (noted $\ctxV{\Delta}$, meaning
that the context is composed of valid types) and a
substitution $\sigma$ is valid, noted $\tytyS{\Delta}{\sigma}{\Gamma}$,
when it substitutes only reducible terms. Similarly, two valid
substitutions $\sigma$ and $\sigma'$ are equal, noted
$\eqtyS{\Delta}{\sigma}{\sigma'}{\Gamma}$, when they substitute
reducibly equal terms.

There are similar notions of validity for terms, equality of
types and equality of terms. For instance, when $A$ is a valid
type, the validity of a term $t$ at $A$ is defined as

\[
{\small
  \inferrule{\forall \Delta\ \sigma \to \metaimply{\ctxV{\Delta}}{\metaimply{\tytyS{\Delta}{\sigma}{\Gamma}}{
        \left \{ \begin{array}{l}
           \tytmR{\ell}{\Delta}{t[\sigma]}{A[\sigma]}{\Type{}} \\
                   \forall \sigma' \to
                   \ \metaimply{\tytyS{\Delta}{\sigma'}{\Gamma} }{\metaimply{\eqtyS{\Delta}{\sigma}{\sigma'}{\Gamma}}{\eqtmR{\ell}{\Delta}{t[\sigma]}{t[\sigma']}{A[\sigma]}{\Type{}} }}
                 \end{array} \right .}}
         }
         {\tytmV{\ell}{\Gamma}{t}{A}{\Type{}}}
       }
\]

%% This notion of validity is a way to enforce stability by substitution
%% for reducible terms.
%
Then, the fundamental lemma is stated with the paranoid variant of the type
system. The lemma is proven using a mutual induction on derivations for well-typed types, terms, equality of
types and equality of terms. We only present the statement on
well-typed terms as we only explain the difference from the proof of
\sidecitet{Abel:POPL2018,pujet:hal-03367052} on the impredicative fragment.
%
Note that because validity of a term can only be defined if the
context and the type are themselves valid, the fundamental lemma
actually provides validity of every component of the typing judgment.
%
\begin{lemma}[Fundamental lemma (for terms)
  \refAgda{LogicalRelation.}{Fundamental}]
  \label{thm:fundamental}
  \
  If\, \( \tytmParaannotated{\Gamma}{t}{A}{\Type{}} \), then there is a level \( \ell \) such that $\ctxV{\Gamma}$,
  \( \tytyV{\ell}{\Gamma}{A}{\Type{}} \) and \( \tytmV{\ell}{\Gamma}{t}{A}{\Type{}} \).
\end{lemma}
\begin{proof}
  The theorem is proven by induction on the paranoid derivation.
  %
  The only cases where the proof differs from
  \sidecitet{Abel:POPL2018,pujet:hal-03367052} is when a rule builds
  an inhabitant of a proof-irrelevant type. In this case, we need
  to show that (i) the proof-irrelevant type is valid, and
  (ii) that the term is well-typed.
  %
  Showing that the term is well-typed is always straightforward, but
  the validity of its type can be more difficult to obtain.

  In the case of the application of functions (Rule \nameref{infrule:pi-elim-Para}), the usual
  predicative way to get the fact that ${\subst{B}{u}}$ is valid
  is to extract it from the information collected by the
  validity of the dependent function type (which is known
  by induction hypothesis).
  %
  In the impredicative case however, we do not get such expansive information
  from the validity of the dependent function type, because validity
  of a type in the impredicative universe cannot recursively mention validity of the codomain, just like reducibility.
  %
  This is precisely where the paranoid assumption comes into play:
  %
  from the induction hypothesis on the additional assumptions, we also
  know that $B$ is valid as a type family over $A$. Since validity is
  defined as reducibility under any valid substitution, we can get the
  reducibility of ${\subst{B}{u}}$.
  %

  This technique also applies to other rules such as \nameref{infrule:fst-Para}
  or \nameref{infrule:snd-Para}.
  %
\end{proof}

\section{Consequences of Normalization}
\label{sec:normalization-consequences}

\subsection{A Paranoid Version of \SetoidCC }

We compensate for this by doing our proof on inference rules that feature 
all the well-formedness hypotheses 
% 
We call the enriched rules \emph{paranoid} in reference to \sidecitet{andrej17:paranoid}.
% 
For instance, the paranoid rule for function abstraction requires hypotheses
of well-formedness for the domain and codomain, whereas the simplified rules
only mention an hypothesis on the function body (rule~\nameref{infrule:pi-intro}).
% 
These supplementary hypotheses are in fact necessary in our proof of the 
fundamental lemma, to recover information that cannot be tracked by the 
weakened model.
% 
Once the fundamental lemma is established, we can prove that most of the 
information in the paranoid rules is in fact redundant, and that the 
simplified rules of \cref{ch:observational} do in fact contain all the 
necessary information.

In order to avoid circularity, we weakened the definition of the
logical relation for dependent products, so that it does not mention
reducibility of the domain and the codomain.
%
In counterpart, we need to prove the fundamental lemma with a
significantly weaker induction hypothesis for proof-irrelevant type
constructors.
%
To remedy to this problem, we define a paranoid variant of the type
system of \SetoidCC that includes seemingly redundant premises in the
typing rules of dependent products and existential types.
%
This variant, noted $\dashPar$, is presented in \cref{fig:SetoidCC-paranoid-typing}. The idea is that every
sub-term of the conclusion must be explicitly typed
in the premises.
%
For instance, the rule for application not only asks for $t$ and $u$ to
be well-typed, but also for domain $A$ and codomain $B$ of the dependent
function type of $t$.

Obviously, when a term is well-typed for the paranoid variant, it is
also well-typed in the economic system. But after proving the
fundamental lemma on the paranoid variant, it becomes possible to show
that the two typing systems are equivalent, leveraging the inversion
lemmas provided by the logical relation.
%
Thus, the additional premises of the paranoid variant help us
prove the fundamental lemma in presence of impredicativity, but end up
being redundant once a sufficient amount of metatheory is established.


\begin{figure}
  \begin{small}
    \begin{mathpar}
 \inferrule[$\Pi$-Form$_p$] {\tytmPara{\Gamma}{A}{\Type[]{}} \\
            \tytmPara{\extctxannotated{\Gamma}{A}{\Type[]{}}}{B}{\Type[]{'}}}
            {\tytmPara{\Gamma}{\Depfunannotated{A}{B}{\Univ}{\Univ}{\Univ'}}{\piRel{\Type}{\Type[]{'}}}}
            {}
\ilabel{infrule:pi-form-Para}
\and
 \inferrule[Fun$_p$]{\tytmPara{\Gamma}{A}{\Type[]{}} \\
   \tytmParaannotated{\extctxannotated{\Gamma}{A}{\Type{}}}{t}{B}{\Type{'}}}
            {\tytmParaannotated{\Gamma}{\lam{A}{t}}{\Depfun{A}{B}}{\piRel{\Type}{\Type[]{'}}}}
\ilabel{infrule:pi-intro-Para}
  \\
  \inferrule[App$_p$]{\tytmPara{\Gamma}{A}{\Type[]{}} \\
    \tytmPara{\extctxannotated{\Gamma}{A}{\Type[]{}}}{B}{\Type[]{'}}
    \\
    \tytmParaannotated{\Gamma}{t}{\Depfun{A}{B}}{\piRel{\Type}{\Type[]{'}}}
            \\ \tytmParaannotated{\Gamma}{u}{A}{\Type{}}}
            {\tytmParaannotated{\Gamma}{t\ u}{\subst{B}{u}}{\Type[]{'}}}
            \ilabel{infrule:pi-elim-Para}
  \\
  \inferrule[$\exists$-Form$_p$]{\tytmPara{\Gamma}{A}{\sProp[i]} \\
             \tytmPara{\extctxannotated{\Gamma}{A}{\sProp}}{B}{\sProp[i]}}
            {\tytmPara{\Gamma}{\Exists{A}{B}}{\sProp[i]}}
            % {\scriptstyle \substack{i \le k \\ j \le k}}
            \ilabel{infrule:exists-form-Para}
            \and
              \inferrule[Fst$_p$]{    \tytmPara{\Gamma}{A}{\sProp[i]} \\
     \tytmPara{\extctxannotated{\Gamma}{A}{\sProp}}{B}{\sProp[i]} \\\tytmParaannotated{\Gamma}{t}{\Exists{A}{B}}{\sProp}}
            {\tytmParaannotated{\Gamma}{\fst{t}}{A}{\sProp}}\ilabel{infrule:fst-Para}
\\
  \inferrule[Snd$_p$]{    \tytmPara{\Gamma}{A}{\sProp[i]} \\
     \tytmPara{\extctxannotated{\Gamma}{A}{\sProp}}{B}{\sProp[i]} \\\tytmParaannotated{\Gamma}{t}{\Exists{A}{B}}{\sProp}}
  {\tytmParaannotated{\Gamma}{\snd{t}}{\subst{B}{\fst{t}}}{\sProp}}
  \ilabel{infrule:snd-Para}
   \\
  \inferrule[Pair$_p$]{
    \tytmPara{\Gamma}{A}{\sProp[i]} \\
     \tytmPara{\extctxannotated{\Gamma}{A}{\sProp}}{B}{\sProp[i]} \\ \tytmParaannotated{\Gamma}{t}{A}{\sProp}
            \\ \tytmParaannotated{\Gamma}{u}{\subst{B}{t}}{\sProp}}
          {\tytmParaannotated{\Gamma}{\pair{t}{u}}{\Exists{A}{B}}{\sProp}}
          \ilabel{infrule:pair-Para}
        \end{mathpar}
        \end{small}
        \caption{\SetoidCC Paranoid Typing Rules (dependent function
          types and existential types only)}
  \label{fig:SetoidCC-paranoid-typing}
\end{figure}

\subsection{Removing Paranoid Premises}
\label{sec:remov-paran-prem}

So far, we established the fundamental lemma on the paranoid
version of the type system, but we would like to work with the more
economic presentation instead.

The first step is to establish the following corollary on the paranoid
typing, by combining the fundamental lemma with the fact that
reducible types are well-formed and reducible terms are well-typed:
%
\begin{corollary}[Typing Validity \refAgda{Typed.Consequences.}{Syntactic}] \label{thm:validity}
  If $\tytmParaannotated{\Gamma}{t}{A}{\Type{}}$ then $\tytmPara{\Gamma}{A}{\Type{}}$ and similarly, if $\eqtmParaannotated{\Gamma}{t}{u}{A}{\Type{}}$ then
  $\tytmPara{\Gamma}{A}{\Type{}}$,
  $\tytmParaannotated{\Gamma}{t}{A}{\Type{}}$ and $\tytmParaannotated{\Gamma}{u}{A}{\Type{}}$.
\end{corollary}
%
And now that we are equipped with this corollary, as well as some
inversion lemmas that we obtained from the logical relation, we know
enough to build a paranoid typing derivation from a
standard derivation:
%
\begin{theorem}[\refAgda{Typed.}{NonParanoidTyping}]
  \label{thm:nonparanoid}
  If $\tytmannotated{\Gamma}{t}{A}{\Type{}}$ then $\tytmParaannotated{\Gamma}{t}{A}{\Type{}}$.
\end{theorem}
\begin{proof}
  The proof is by induction on the typing derivation. The
  correspondence between the two systems is one-to-one except for the
  additional paranoid assumptions.
  %
  In all cases, we can get them from \cref{thm:validity} and from
  inversion lemmas, such as the fact that $A$ and $B$ are well-typed
  whenever $\Depfun{A}{B}$ is.
\end{proof}

Thus, we also have the fundamental lemma for the economic type system:
%
\begin{corollary}[Fundamental lemma on the economic type system]

  \

  If\, \( \tytmannotated{\Gamma}{t}{A}{\Type{}} \), then there is a level \( \ell \) such that $\ctxV{\Gamma}$,
  \( \tytyV{\ell}{\Gamma}{A}{\Type{}} \) and \( \tytmV{\ell}{\Gamma}{t}{A}{\Type{}} \).
\end{corollary}


A direct consequence of the fundamental lemma is that any well-typed
term has a weak-head normal form.
%
Another consequence is that any closed
term of type $\Nat$ reduces to a whnf of type $\Nat$. Thus, to
obtain canonicity for the integers of \SetoidCC, we just need to
know that there is no neutral term of type $\Nat$ in an empty context
\refAgda{Typed.Consequences.}{Canonicity}.

Unfortunately, as in \sidecite{pujet:hal-03367052}, the logical relation
is not sufficient to establish this. In fact, as we will see in \cref{sec:analys-norm-proof}, there is a good reason for that: the
consistency and canonicity theorems for \SetoidCC have a remarkably high
proof-theoretical strength due to impredicativity, whereas the
logical relation argument can be developed in a predicative
meta-theory.
%
In \cref{sec:cons-seto-model}, we will supplement
 the normalization result with
a model of \SetoidCC in an impredicative meta-theory, from which
we will deduce the consistency and canoncity theorems.

\subsection{Decidability of Conversion}

Besides proving weak-head normalization of well-typed terms, the
main point of the fundamental lemma is to be able to show that
conversion is decidable in $\SetoidCC$.
%
To do so, we define an algorithmic relation
$\algoeqtmannotated{\Gamma}{t}{u}{A}{\Type{}}$ which is supposed to
simulate convertibility of two terms $t$ and $u$, while being
easier to decide \refAgda{}{Conversion}.
%
This algorithmic conversion keeps track of the relevance information,
and uses it to either give an immediate answer in case \( t \) and
\( u \) are irrelevant, or compute their weak-head normal forms, then
compare their head constructor, and possibly apply the algorithm
recursively in case \( t \) and \( u \) are relevant.
%
Correctness of this algorithmic conversion is direct as the rules used are
subsumed by the general conversion judgement \refAgda{Conversion.}{Soundness}.

Showing that the algorithmic conversion is also complete is more complex.
%
It basically amounts to replaying the fundamental lemma with a definition of the logical
relation that uses algorithmic conversion instead of the general conversion
\refAgda{Conversion.Consequences.}{Completeness}.
%
In our formal proof, we follow \sidecitet{Abel:POPL2018} in factoring the two
instances of the fundamental lemma by defininig a generic interface for both
algorithmic conversion and typed conversion, and using this interface
in the definition of the logical relation \refAgda{Typed.}{EqualityRelation}.

\begin{theorem}[Equivalence of conversion and algorithmic conversion]
  \label{thm:algoconv}
    Given two terms $t$ and $u$ such that
    $\tytmannotated{\Gamma}{t}{A}{\Type{}}$ and
    $\tytmannotated{\Gamma}{u}{A}{\Type{}}$, we have that

    $$ \eqtmannotated{\Gamma}{t}{u}{A}{\Type{}} \Longleftrightarrow\algoeqtmannotated{\Gamma}{t}{u}{A}{\Type{}}.$$
\end{theorem}

It still remains to provide a decision procedure for the algorithmic
conversion \refAgda{Conversion.}{Decidable}.
%
Given two terms $t$ and $u$ well-typed at $A : \Type{}$ in context
$\Gamma$, we can apply the fundamental lemma plus the reflexivity of
the logical relation to get the fact that $\eqtmR{\ell}{\Gamma}{t}{t}{A}{\Type{}}$
and similarly for $u$.
%
Because reducibly equal terms are also algorithmically convertible, we
have $\algoeqtmannotated{\Gamma}{t}{t}{A}{\Type{}}$ (and similarly for $u$).
%
Then the decision procedure is done by double induction on the proofs
that $t$ and $u$ are reflexive for the algorithmic conversion.
%
The idea is that $t$ and $u$ are convertible if and only if the two reflexive proofs are the same.

Note that the proof that $t$ is algorithmically equal to itself
contains the fact that $t$ strongly normalizes, because
$t$ can be recursively put in whnf.

\subsection{Decidability of Typing}

\shepherd{TODO}

\section{Analysis of the Normalization Proof}
\label{sec:analys-norm-proof}

The informal proof presented in \cref{sec:logic-relat-with}, and
formalized in \Agda, outlines the construction of a normalization
model in a rather powerful metatheory: Martin-LÃ¶f Type Theory extended
with induction-recursion.
%
However, in order to control the computational complexity of the proof terms
more finely, we can try to weaken this metatheory as much as possible.

\subsection{The Computational Power of \SetoidCC}

To establish a simple lower bound, we can start by noting that Martin-LÃ¶f
Type Theory is a subset of \SetoidCC. Indeed, dependent products and
the universes are handled in the exact same way, and \sidecitet{pujet:hal-03367052} explain
how to deal with inductive tyes.
%
Therefore, a normalization proof for \SetoidCC can also act as a
normalization proof for \MLTT.
%
And since normalization entails consistency for \MLTT, it follows that our
meta-theory needs to be at least as strong as \MLTT---in fact, it turns out to be exactly what we need.

\begin{theorem}\label{setoidCC-in-MLTT}
  \MLTT with \( n + 4 \) universes can prove normalization of \SetoidCC with
  \( n \) universes.
\end{theorem}

We defer the proof of this theorem to \cref{sec:fitt-norm-proof} and
first analyse what can be deduced from it.

\begin{corollary}
  Normalization for \SetoidCC is equivalent to normalization for \MLTT
  over a weak fragment of arithmetic.
\end{corollary}

Keep in mind that we cannot hope for a similar result for consistency or
canonicity: while consistency and canonicity for \MLTT follow from
normalization, proving that \SetoidCC is consistent requires the increased
power of impredicativity.

\cref{setoidCC-in-MLTT} also provides us with some
control over the integer functions of \SetoidCC:

\begin{corollary}\label{integer-functions}
  \SetoidCC and \MLTT can express the exact same integer functions
  as closed terms of type \( \Fun{\Nat}{\Nat} \).
\end{corollary}

\begin{proof}
  As already explained, functions from \MLTT can simply be embedded in
  \SetoidCC.
  %
  For the converse direction, note that any closed term \( f \) of
  type \( \Fun{\Nat}{\Nat} \) in \SetoidCC only mentions a finite number of
  universes.
%
  Thus, \cref{setoidCC-in-MLTT} provides us with a
  normalization function for the corresponding fragment of \SetoidCC,
  that computes a normal form when applied to \( f n \) where \( n \)
  is a closed syntactic integer.
%
  If we compose it with a function that sends normal forms to the
  adequate integer, %(or 0 if the normal form is non-canonical)
  we obtain an integer function in \( \MLTT \), and canonicity ensures
  that it represents the same function as the original \SetoidCC term.
\end{proof}

%
\cref{integer-functions} might come off as a surprise, since \SetoidCC is equipped with an impredicative universe,
and impredicativity generally adds a great deal of proof-theoretical
strength!
%
And \SetoidCC does possess this logical power, in fact. Indeed, it can
represent many more functions as \( \sProp \)-valued functional relations
than \MLTT. But this corollary shows that there is no way to extract them
in the proof-relevant fragment, even though we have access to elimination
principles for false propositions and equalities (using casts).
%
Thus, the logical power of impredicativity is locked inside of \( \sProp \).
%
This is in stark contrast with the Calculus of Inductive Constructions,
in which it is possible to define closed terms of type \( \Fun{\Nat}{\Nat} \) that leverage
the power of impredicativity, using a principle called large elimination
of singleton inductive types\footnote{The standard technique is to use
the accessibility predicate as defined in \url{https://coq.inria.fr/library/Coq.Init.Wf.html}.}.
%
We discuss this principle with more detail in \cref{sec:setoidcc-with-an}.

\subsection{Fitting the Normalization Proof in MLTT}
\label{sec:fitt-norm-proof}

In this subsection, we give a proof of \cref{setoidCC-in-MLTT}, by
encoding the logical relation sketched in \cref{sec:logic-relat-with}
without induction-recursion.
%
This argument has been formalized in \Agda.\footnote{The formalization is in the
  folder \texttt{logrel-wo-ind-rec}. It is done for \MLTT and not
full \SetoidCC, but it does not make a difference as the crux is in
the definition of the relation without induction-recursion.}
%
For the remainder of this section, we work in plain \MLTT with a deep
embedding of \SetoidCC.
%
To avoid confusion between the meta-theory and the object theory, we
will use \Agda-style notations for the meta-theory.

From a bird's eye perspective, the normalization proof builds a model
of \SetoidCC in which well-formed types are interpreted as proof-relevant
predicates on untyped terms, and induction-recursion is used to construct a
universe in the model: we define the inductive predicate of reducible types
simultaneously with recursive functions that associates reducibility
predicates to a reducible type.
%
On closer inspection, we realize that the proof still works if the
reducibility predicate of a universe lives one universe higher than the
reducibility predicates of the types it contains. This allows us to use
\emph{small} induction-recursion, which can be replaced by functional
relations that only require simple indexed inductive types
\sidecite{small-ind-rec}.

\cref{fig:logrel-ind} showcases what the relation-based definition
looks like.
%
For all \( \ell \le n \), we define an inductive relation \( \AgdaData{R}^\ell \) between
a context, an untyped term \( t \) (the reducible type), its sort, a predicate \( P_{=} \) of
types that are convertible to \( t \), a predicate \( P_{t} \) which is the
reducibility predicate associated to \( t \), and a binary relation
\( P_{t=} \) that encodes convertibility of terms in \( P_{t} \).
%
The logical relation \( \Vdash_\ell \) is then defined in terms of \( \AgdaData{R}^\ell \).
%
The inductive relation features eight constructors that are each
defined in \cref{sec:logic-relat-with}, including \( \Rpi \)
which recursively calls the logical relation.
%% , plus an eighth one that ensures
%% \( \AgdaData{R}^\ell \) contains all the relations \( \AgdaData{R}^{\ell'} \) for \( \ell' < \ell \).

Note that the logical relation is built in stages:
\begin{itemize}
\item We first define an inductive relation
  \( \AgdaData{R}^0 \) that has cases for dependent products and all base types, except
  for proof-relevant universes: it only accounts for inhabitants of
  \( \Type_0 \).
  %
\item From \( \AgdaData{R}^0 \), we define a reducibility predicate \( P_{\Type_0} \) for
  \( \Type_0 \): a term \( t \) is in \( P_{\Type_0} \) if there exist
  \( P_=, P_t, P_{t=} \) such that \( R^0(t,P_=, P_t, P_{t=}) \).
  %
  Now that we have a predicate for \( \Type_0 \), we can define a relation
  \( \AgdaData{R}^1 \) that accounts for inhabitants of \( \Type_1 \). Of course, since
  \( P_{\Type_0} \) lives in \( \AgdaSet{1} \), \( \AgdaData{R}^1 \) will land in \( \AgdaSet{2} \).
\item We repeat this process \( n \) times to obtain a relation \( \AgdaData{R}^n \)
  that handles our \( n \) universes.
\end{itemize}
Each step of this process requires an additional meta-theoretical universe
level.
%
This is only natural, since we are proving normalization for a theory that
subsumes \( \MLTT_n \), a property from which we can deduce the consistency
of \( \MLTT_n \).
%
Therefore, GÃ¶del's incompleteness theorem applies and guarantees that
we need more universes in the meta-theory than in the fragment we
consider.


\begin{figure}
  \begin{small}
\begin{flalign*}
  \begin{array}{lcl}
   \AgdaData{R}^\ell         & : & \Context \to \Term \to (\Term \to \AgdaSet{\ell}) \to (\Term \to \AgdaSet{\ell}) \to (\Term \to \Term \to \AgdaSet{\ell}) \to \AgdaSet{\ell+1} \\
   \AgdaData{R}^\ell         & \bnfis & \Rne : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\mathsf{ne}} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\mathsf{ne}} t \equiv \_)\ (\Gamma\ \Vdash_{\mathsf{ne}} \_ : t)\ (\Gamma\ \Vdash_{\mathsf{ne}} \_ \equiv \_ : t)\\
               & \sep & \RU : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\mathsf{U}} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\mathsf{U}} t \equiv \_)\ (\Gamma\ \Vdash_{\mathsf{U}} \_ : t)\ (\Gamma\ \Vdash_{\mathsf{U}} \_ \equiv \_ : t)\\
               & \sep & \Rnat : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Nat} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\Nat} t \equiv \_)\ (\Gamma\ \Vdash_{\Nat} \_ : t)\ (\Gamma\ \Vdash_{\Nat
                          } \_ \equiv \_ : t)\\
               & \sep & \Rpi : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Pi} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\Pi} t \equiv \_)\ (\Gamma\ \Vdash_{\Pi} \_ : t)\ (\Gamma\ \Vdash_{\Pi} \_ \equiv \_ : t)\\
               & \sep & \ROmega : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Omega} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\Omega} t \equiv \_)\ (\Gamma\ \Vdash_{\Omega} \_ : t)\ (\Gamma\ \Vdash_{\Omega} \_ \equiv \_ : t)\\
               & \sep & \Rforall : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\Pi \mathsf{i}} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\Pi \mathsf{i}} t \equiv \_)\ (\Gamma\ \Vdash_{\Pi \mathsf{i}} \_ : t)\ (\Gamma\ \Vdash_{\Pi \mathsf{i}} \_ \equiv \_ : t)\\
               & \sep & \Rexists : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\exists} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\exists} t \equiv \_)\ (\Gamma\ \Vdash_{\exists} \_ : t)\ (\Gamma\ \Vdash_{\exists} \_ \equiv \_ : t)\\
               & \sep & \Rempty : \forall\ \Gamma\ t \to
                          (\Gamma \Vdash_{\bot} t) \to
                          \AgdaData{R}^\ell\ \Gamma\ t\ (\Gamma\ \Vdash_{\bot} t \equiv \_)\ (\Gamma\ \Vdash_{\bot} \_ : t)\ (\Gamma\ \Vdash_{\bot} \_ \equiv \_ : t)\\
               %% & \sep & \Remb : \forall\ \Gamma\ t\ P_=\ P_t\ P_{t=} \to \AgdaData{R}^{\ell'}\ \Gamma\ t\ P_=\ P_t\ P_{t=} \to \AgdaData{R}^\ell\ \Gamma\ t\ P_=\ P_t\ P_{t=} \qquad \text{for all } \ell' < \ell
  \end{array} &&
\end{flalign*}

\begin{flalign*}
& \text{All the functions whose name contains } \Vdash_{\mathsf{ne}} \, , \ \Vdash_{\mathsf{U}} \, , \ \Vdash_{\Nat} \, ,\ \Vdash_{\Pi} \, ,\ \Vdash_{\Pi \mathsf{i}} \, ,\ \Vdash_{\exists} \, ,\ \Vdash_{\bot}
\text{ are as defined in \cref{sec:logic-relat-with}.} &
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ : \_ & : & (\Gamma : \Context) \to (t : \Term) \to (A : \Term) \to (s : \Sort) \to \AgdaSet{\ell+1} \\
\Gamma \Vdash_\ell t : s & = & (P_= : \Term \to \AgdaSet{\ell}) \times (P_t : \Term \to \AgdaSet{\ell}) \\ && \times\ (P_{t=} : \Term \to \Term \to \AgdaSet{\ell}) \times (\AgdaData{R}^\ell\ \Gamma\ t\ s\ P_=\ P_t\ P_{t=})
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ \equiv \_ : \_ & : & (\Gamma : \Context) \to (A : \Term) \to (B : \Term) \to (s : \Sort) \to \{ \Gamma \Vdash_\ell A : s \} \to \AgdaSet{\ell} \\
\Gamma \Vdash_\ell A \equiv B : s & = & P_=\ B
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ : \_ : \_ & : & (\Gamma : \Context) \to (t : \Term) \to (A : \Term) \to (s : \Sort) \to \{ \Gamma \Vdash_\ell A : s \} \to \AgdaSet{\ell} \\
\Gamma \Vdash_\ell t : A : s & = & P_t\ t
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \begin{array}{lcl}
\_ \Vdash_\ell \_ \equiv \_ : \_ : \_ & : & (\Gamma : \Context) \to (t\ u : \Term) \to (A : \Term) \to (s : \Sort) \to \{ \Gamma \Vdash_\ell A : s \} \to \AgdaSet{\ell} \\
\Gamma \Vdash_\ell t \equiv u : A : s & = & P_{t=}\ t\ u
  \end{array} &&
\end{flalign*}
\end{small}
  \caption{Inductive encoding of the logical relation}
  \label{fig:logrel-ind}
\end{figure}

Constructing this universe of reducible types is only the first half
of the normalization proof. To complete the construction, we also need
to encode validity without induction-recursion in \( \MLTT \) and
prove the fundamental lemma.
%
We do not develop this here as it uses a similar construction and
refer the interested reader to the formalization for the full proof.

\section{Semantics of \SetoidCC}
\label{sec:cons-seto-model}

In this section, we turn to the construction of the
\emph{standard} set-theoretic model of \SetoidCC.
%
This model serves two purposes: to show consistency of our theory on
the one hand, and to ensure that \SetoidCC is a good internal language for
set theory on the other hand.

\subsection{Deriving Consistency from a Model}

As we already mentioned, the normalization model that we built in
\cref{sec:logic-relat-with} is not strong enough to obtain consistency
and canonicity for \SetoidCC. Indeed, this model interprets the
proof-irrelevant proposition \( \bot \) as the set of syntactical
terms with type \( \bot \), which does not grant any kind of control
over its proofs in the empty context.

This was already the case in the work of \sidecitet{pujet:hal-03367052} on
\SetoidCC, so they supplemented their normalization proof with a model
in the category of sets (presented as setoids), using
induction-recursion to interpret universes as sets of codes \emph{Ã 
  la Tarski} defined mutually with eliminators and coercion functions.
%
In that model, \( \bot \) is actually interpreted as the empty set, which
proves consistency of \SetoidCC: we can use it to show that there is
no term of type \( \bot \) in the empty context.

\subsection{\SetoidCC as an Internal Language for Sets}

While the construction of \sidecitet{pujet:hal-03367052} does show consistency of
\SetoidCC, it arguably falls short of presenting \SetoidCC as an internal
language for classical mathematics.
%
Indeed, the inductive-recursive construction of the universe of codes is
designed to only account for the codes that come from the syntax of
\SetoidCC. In other words, the following statement
\[
\Pi(A : \Type)(x\ y\ z : A).\,\Fun{\Fun{\Obseq[A]{x}{y}}{\Obseq[A]{y}{z}}}{\Obseq[A]{x}{z}}
\]
only states transitivity of equality for sets that are built from
the type formers of \SetoidCC, when interpreted in their model.

This state of affairs is obviously not ideal, as readers who are not
willing to accept \SetoidCC as a new foundation of mathematics would
probably be more inclined to use it if they knew that any theorem they
proved in \SetoidCC is also true for classical set theory.
%
Thus, instead of replicating this construction in an impredicative
meta-theory (to be able to interpret \( \sProp \)), we seize the
opportunity to solve two problems at once, and present a model in classical
set theory that gives a much more sensible meaning to statements in \SetoidCC
while still being able to show consistency.

\subsection{Constructing the Set-theoretical Model}

We work in ZF set theory with a countable hierarchy of Grothendieck universes
\( \sV_0, \sV_1, ... \)
%
We call \( \Two \) the lattice of truth values (or in other words, the
booleans), with \( \independent \) as its minimal value,
and given \( p \in \Two \) we write \( \val\ p \) for the
corresponding sub-singleton set \( \{ x \in \{ * \}\ |\ p \} \).
%
Even though we stay in the world of set theory for the duration of
this section, we remain type theorists, and as such
we use dependent products, dependent sums and inductive types in this
section. They should be understood as their standard interpretation in
the set-theoretic model of type theory.
%
In an attempt to minimize confusion, we change our notations a little
bit: we use \( (a \in A) \to (B\ a) \) for the set-theoretic dependent product,
\( (a \in A) \times (B\ a) \) for the the set-theoretic dependent sum and
\( \metanat \) for the set-theoretic integers.

The central ingredient of our standard model is the hierarchy of universes of
codes \( \sU_0, \sU_1... \), that are used to interpret the proof-relevant universes
of \SetoidCC.
%
It is tempting to define them directly as Grothendieck universes
(\( \sU_i = \sV_i \)), but unfortunately this does not work: in \SetoidCC,
type constructors are injective with respect to the observational equality
(for instance, \( \Obseq{(\Fun{A}{B})}{(\Fun{A'}{B'})} \) implies that
\( \Obseq{A}{A'} \) and \( \Obseq{B}{B'} \)) while set-theoretic function
spaces collapse too much information for this to be possible
(\( \Fun{\bot}{\Bool} \) and \( \Fun{\bot}{\Nat} \) are identical).

This is not too difficult to fix, however: we simply need to add a bit of
information to the codes in the universe, so that we can keep track of how
we constructed any given set. The most natural way to do this from a type
theorist's perspective is to build an inductive predicate \( \ \Upred \) over
\( \sV_i \) and then interpret the universes of \SetoidCC as
\( \sU_i = (X \in \sV_i)\times(\Upred\ X) \), as described in \cref{fig:model}.
%
Note how the constructor \( \codeemb \) builds a proof of \( \ \Upred\ X \)
for any (small) set \( X \), so that statements that quantify over
\( \Type_i \) apply to all appropriately-sized sets of the model.
%
This also implies that there might be several codes for the
same set: for instance \( (\Fun{\Nat}{\Nat} \ ;\ \codeemb) \) and
\( (\Fun{\Nat}{\Nat} \ ;\ \codepiuu\ ...) \) are both codes for
the set \( \Fun{\Nat}{\Nat} \), but only the second one remembers that it
has been built as a function space.

Remark that the natural equality between codes matches closely the equality
of \SetoidCC: two codes can only be equal if they pack the same witness of
\( \Upred \), which means that they have been built in the same way.
Moreover, if we compare two codes of the form
\( (X \to Y ;\ \codepiuu\ ...) \), the equality of the second member means that
the domains and the codomains of the function spaces are equal---we recover the injectivity of
the type formers.

\begin{figure}
   \begin{small}
\begin{flalign*}
  \begin{array}{lcl}
   \Upred         & : & \Fun{\sV_i}{\sV_{i+1}}\\
   \Upred         & \bnfis & \tm{\codeemb}{\Fun{(X \in \sV_i)}{\Upred\ X}}\\
%                     & \sep & \tm{\codenat}{\sU_i} & \qquad & \text{if i = 0}\\
                     & \sep & \tm{\codepiuu}{\{ j, k \in \metanat\ |\ \mathrm{max}(j, k) \le i\} \to (A \in \sV_j) \to (A_\varepsilon \in \Upred\ A)} \\
                  & & \qquad \to (B \in (A \to \sV_k)) \to (B_\varepsilon \in ((a \in A) \to \Upred\ (B\ a)))\\
                  & & \qquad \to \Upred\ {((a \in A) \to B\ a)}\\
                     & \sep & \tm{\codepisu}{(A \in \ssProp)} \\
                  & & \qquad \to (B \in (\val\ A \to \sV_i)) \to (B_\varepsilon \in ((a \in \val\ A) \to \Upred\ (B\ a)))\\
                  & & \qquad \to \Upred\ {((a \in \val\ A) \to B\ a)}\\
                     & \sep & \tm{\codeU}{\Fun{\{ j \in \metanat\ |\ j < i\}}{\Upred\ \sU_j}} \\
                     & \sep & \tm{\codesProp}{\Upred\ \ssProp} \\
  \end{array} &&
\end{flalign*}

\begin{flalign*}
  \hspace{5pt}\sU_i \quad := \quad (X \in \sV_i)\times(\Upred\ X) &&
\end{flalign*}
%
\begin{flalign*}
  &\hspace{5pt}\el \quad : \quad \sU_i \to \sV_i&\\
  &\hspace{5pt}\el\ (X\ ;\ X_\varepsilon)\ :=\ X&
\end{flalign*}
\end{small}
  \caption{The universe of codes, defined with set-theoretic inductive types}
  \label{fig:model}
\end{figure}

\subsection{Interpreting the Syntax of \SetoidCC}

Now that we know how to deal with the universes, it only
remains to spell out the interpretation of the full syntax of \SetoidCC.
%
In the standard fashion~\sidecite{hofmann95}, we define interpretation functions that send
\begin{itemize}
  \item a context \( \Gamma \) to a set \( \mctx{\Gamma} \), and
  \item a pair of a term \( t \) and a context \( \Gamma \) to a set \( \mty{t} \) indexed over \( \mctx{\Gamma} \).
\end{itemize}
Since these functions are defined on raw syntax without any guarantee of
well-typedness, we cannot expect every term to have a sensible interpretation
so we need to make them \emph{partial} functions. We will be able to prove
that all well-typed terms admit an interpretation \textit{a posteriori} by
induction on the derivations.

As usual, contexts are interpreted as telescopes:
\( \mctx{\emptyctx} = \{ * \} \) and
\( \mctx{\extctx{\Gamma}{A}} = (\rho : \mctx{\Gamma})\times (\mty{A}\ \rho) \)
when \( \mty{A} \) is defined.
%
Given \( \rho \in \mctx{\Gamma} \) and a variable \( x \) that appears in
\( \Gamma \), we write \( \rho(x) \) for the corresponding projection.
%
The interpretation of the terms is defined in
\cref{fig:interpretation}. The proof-relevant fragment unsurprisingly
follows the standard interpretation of dependent type theory, and the
observational equality is interpreted as the extensional equality of
set-theory.
%
Note how the irrelevant proofs are all interpreted as \( * \), the inhabitant
of the singleton set. This forces us to interpret cast as doing nothing, and \( \emptyrec{A}{t} \) is not even interpreted since it can only be formed in inconsistent contexts, which are empty in the model.

In order to prove the soundness of our interpretation, we need to extend it to weakenings
and substitutions between contexts.
%
Assume \( \Gamma \) and \( \Delta \) are a syntactical contexts, and \( A \) and \( t \)
are syntactical terms.
%
In case \( \mctx{\Gamma, \tm{x}{A}, \Delta} \) and \( \mctx{\Gamma, \Delta} \) are well-defined,
let \( \pi_A \) be the projection:
{\small
\[
  \begin{split}
  \pi_A : \mctx{\Gamma, \tm{x}{A}, \Delta} & \to \mctx{\Gamma, \Delta} \\
  (\vec{x_\Gamma}, x_A, \vec{x_\Delta}) & \mapsto (\vec{x_\Gamma}, \vec{x_\Delta}).
  \end{split}
\]
}
In case \( \mctx{\Gamma, \subst{\Delta}{t}} \) and \( \mctx{\Gamma, \tm{x}{A}, \Delta} \) are
well-defined, we define the function \( \sigma_t \) by:
{\small
\[
  \begin{split}
    \sigma_t : \mctx{\Gamma, \subst{\Delta}{t}} & \to \mctx{\Gamma, \tm{x}{A}, \Delta} \\
    (\vec{x_\Gamma}, \vec{x_\Delta}) & \mapsto (\vec{x_\Gamma}, \mty{t}\ \vec{x_\Gamma}, \vec{x_\Delta}).
  \end{split}
\]
}
%
\begin{lemma}[Weakening]\label{lem:weakening}
  \( \pi_A \) is the semantic counterpart to the weakening of \( A \): for all terms \( u \),
  when both sides are well defined, we have:
  {\small
\[
    \mty[{\Gamma, \tm{x}{A}, \Delta}]{u} = \mty[{\Gamma, \Delta}]{u} \circ \pi_A
  \]}
\end{lemma}
%
\begin{lemma}[Substitution]\label{lem:substitution}\ \( \sigma_t \) is the semantic counterpart to the substitution by \( t \):
  for all terms \( u \), when both sides are well defined, we have:
  {\small
\[
    \mty[{\Gamma, \subst{\Delta}{t}}]{\subst{u}{t}} = \mty[{\Gamma, \tm{x}{A}, \Delta}]{u} \circ \sigma_t
  \]}
\end{lemma}

\begin{theorem}[Soundness of the Standard Model]
  \
  \begin{enumerate}
    \item If\, \( \wfctx{\Gamma} \) then \( \mctx{\Gamma} \) is defined.
    \item If\, \( \tytm{\Gamma}{A}{\sProp} \) then \( \mty{A}\ \rho \) is a proposition for all \( \rho \in \mctx{\Gamma} \).
    \item If\, \( \tytm{\Gamma}{A}{\Type_i} \) then \( \mty{A}\ \rho \) is in \( \sU_i \) for all \( \rho \in \mctx{\Gamma} \).
    \item If\, \( \tytmannotated{\Gamma}{t}{A}{\sProp} \) then \( \mty{A}\ \rho \) is true for all \( \rho \in \mctx{\Gamma} \).
    \item If\, \( \tytmannotated{\Gamma}{t}{A}{\Type_i} \) then \( \mty{t}\ \rho \) is in \( \el\ (\mty{A}\ \rho) \) for all \( \rho \in \mctx{\Gamma} \).
    \item If\, \( \eqtm{\Gamma}{t}{u}{A} \) then \( \mty{t} = \mty{u} \).
  \end{enumerate}
\end{theorem}
\begin{proof}
  By induction on the typing derivations, using \cref{lem:weakening,lem:substitution}.
\end{proof}


\begin{figure}
  \begin{small}
\[
\begin{array}{rcl}
  \mty{x}\ \rho & := & \rho(x) \\
  \mty{\Type_i}\ \rho & := & \langle \sU_i\ ;\ \codeU\ i \rangle \\
  \mty{\sProp[i]}\ \rho & := & \langle \ssProp\ ;\ \codesProp \rangle\\
  \mty{\Depfunannotated[y]{F}{G}{}{\Type_j}{\Type_k}}\ \rho & := & \big\langle (x \in\el\ \mty{F}\ \rho)\to (\el\ \mty[{\extctx[x]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ x\rangle)\\
& & ;\ \codepiuu\ j\ k\ (\mty{F}\ \rho)\ (x \mapsto \mty[{\extctx[x]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ x\rangle) \big\rangle\\
  \mty{\Depfunannotated[y]{F}{G}{}{\sProp}{\Type_k}}\ \rho & := & \big\langle (x \in\val\ \mty{F}\ \rho)\to (\el\ \mty[{\extctx[x]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ x\rangle)\\
& & ;\ \codepisu\ (\mty{F}\ \rho)\ (x \mapsto \mty[{\extctx[x]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ x\rangle) \big\rangle\\
  \mty{\Depfunannotated[y]{F}{G}{}{\Type_j}{\sProp}}\ \rho & := & \forall x \in (\el\ \mty{F}\ \rho),\ \mty[{\extctx[x]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ x\rangle\\
  \mty{\Depfunannotated[y]{F}{G}{}{\sProp}{\sProp}}\ \rho & := & (\mty{F}\ \rho) \Rightarrow (\mty[{\extctx[x]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ *\rangle)\\
  \mty{\lam[x]{F}{t}}\ \rho & := & x \mapsto (\mty[{\extctx[x]{\Gamma}{F}}]{t}\ \langle \rho\ ;\ x\rangle)\\
  \mty{t\ u}\ \rho & := & (\mty{t}\ \rho)\ (\mty{u}\ \rho)\\
  \mty{\Nat}\ \rho & := & \codeemb\ \metanat \\
  \mty{\zero}\ \rho & := & \metazero \\
  \mty{\suc{t}}\ \rho & := & \metasuc{(\mty{t}\ \rho)} \\
  \mty{\natrec{P}{t_0}{t_S}{n}}\ \rho & := & \metanat\mathrm{-elim}(\el \circ (\mty{P}\ \rho), \mty{t_0}\ \rho, \mty{t_S}\ \rho, \mty{n}\ \rho)\\
  \mty{\Exists[y]{F}{G}}\ \rho & := & (\mty{F}\ \rho) \land (\mty[{\extctx[y]{\Gamma}{F}}]{G}\ \langle \rho\ ;\ * \rangle) \\
  \mty{\pair{t}{u}}\ \rho & := & * \\
  \mty{\fst{t}}\ \rho & := & * \\
  \mty{\snd{t}}\ \rho & := & * \\
  \mty{\Empty}\ \rho & := & \independent \in \Two \\
  \mty{\emptyrec{A}{t}}\ \rho & := & \text{undefined} \\
  \mty{\Obseq[A]{t}{u}}\ \rho & := & \sEq{\mty{t}\ \rho}{\mty{u}\ \rho} \qquad\text{in }\el(\mty{A}\ x) \\
  \mty{\refl{t}{A}}\ \rho & := & * \\
  \mty{\transport{F}{t}{G}{u}{t'}{e}}\ \rho & := & * \\
  \mty{\cast{A}{B}{e}{t}}\ \rho & := & t\\
  \mty{\castrefl{A}{t}}\ \rho & := & *
\end{array}
\]
\end{small}
  \caption{Interpretation of \SetoidCC in the Standard Model}
  \label{fig:interpretation}
\end{figure}

\subsection{Consequences of the Model}

From the soundness theorem and the interpretation of \( \bot \) being empty,
the consistency of \SetoidCC is immediate:

\begin{theorem}[Consistency]
  There are no proofs of \( \bot \) in the empty context.
\end{theorem}

Furthermore, by inspecting the normal forms provided by the normalization
theorem, we realize that \( \castName \) is the only way to build a neutral
term in the absence of variables. But having a stuck \( \castName \) requires
having an equality proof between two incompatible types, which contradicts
consistency.

\begin{theorem}
  There are no neutral terms in the empty context.
\end{theorem}

From there, we deduce the canonicity theorem: all integers reduce to
standard integers in the empty context. Thus, our modified model still allows
us to establish all of the important meta-theoretical properties.

On the other hand, it seems difficult to measure to what extent our
set-theoretic model presents
\SetoidCC as a good internal language for sets. Compared to the universe
construction of \sidecitet{pujet:hal-03367052}, we gain the property that every set belongs
to a universe---meaning that theorems which quantify over \( \Type_i \)
will apply to all sufficiently small sets.
%
Of course, this does not prevent us from proving some theorems that are
obviously false in classical mathematics, such as the
injectivity of type formers. But we would argue that all such results
are artifacts of the choice of encodings, and not meaningful
mathematical statements.

